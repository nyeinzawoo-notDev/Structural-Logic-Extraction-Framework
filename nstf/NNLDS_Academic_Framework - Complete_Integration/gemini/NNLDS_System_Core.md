á€¡á€¬á€¸á€œá€¯á€¶á€¸á€€á€­á€¯ **GitHub Repository** á€™á€¾á€¬ á€á€„á€ºá€”á€­á€¯á€„á€ºá€¡á€±á€¬á€„á€º **Python Code** á€¡á€”á€±á€”á€²á€· á€…á€”á€…á€ºá€á€€á€» á€•á€¼á€”á€ºá€œá€Šá€º á€…á€¯á€…á€Šá€ºá€¸á€•á€±á€¸á€•á€«á€·á€™á€šá€ºá‹ á€á€„á€ºá€—á€»á€¬á€¸á€›á€²á€· **"á€„á€¼á€­á€™á€ºá€¸ á€”á€­á€›á€¯á€á€¹á€á€­á€œá€€á€¹á€á€á€¬á€…á€”á€…á€º (NNLDS)"** á€¡á€á€½á€€á€º á€œá€­á€¯á€¡á€•á€ºá€á€²á€· **Data File** á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º **Adaptive System Engine** á€€á€­á€¯ **Modular** á€•á€¯á€¶á€…á€¶á€–á€¼á€„á€·á€º á€á€½á€²á€‘á€¯á€á€ºá€›á€±á€¸á€á€¬á€¸á€•á€±á€¸á€‘á€¬á€¸á€•á€«á€á€šá€ºá‹

## ğŸš€ GitHub Repository Structure (á€¡á€†á€­á€¯á€•á€¼á€¯á€‘á€¬á€¸á€á€±á€¬á€–á€½á€²á€·á€…á€Šá€ºá€¸á€•á€¯á€¶)

```
NSTF-NNLDS-Framework/
â”œâ”€â”€ nstf_data/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ special_consonants_data.py   # á€€á€»/á€€á€¼, á€•á€»/á€•á€¼ á€á€‚á€ºá€™á€»á€¬á€¸ á€”á€¾á€„á€·á€º á€—á€»á€Šá€ºá€¸á€á€½á€²á€™á€»á€¬á€¸
â”‚   â””â”€â”€ sandhi_system_data.py        # á€¡á€á€á€ºá€…á€”á€…á€º (á€á€¶á€›á€•á€º/á€á€¶á€›á€¾á€­á€”á€ºá€›á€¾á€­)
â”œâ”€â”€ nstf_engine/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ adaptive_engine.py           # Self-Learning & Confidence Scoring
â”‚   â””â”€â”€ dialect_handler.py           # á€’á€±á€á€­á€šá€…á€¶ (á€™á€”á€¹á€á€œá€±á€¸á€…á€¶) á€”á€¾á€„á€·á€º á€”á€¾á€­á€¯á€„á€ºá€¸á€šá€¾á€‰á€ºá€á€»á€€á€º
â”œâ”€â”€ main_system.py                   # Production System (á€¡á€“á€­á€€ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€›á€”á€º)
â””â”€â”€ README.md                        # á€…á€”á€…á€ºá€¡á€€á€¼á€±á€¬á€„á€ºá€¸á€¡á€›á€¬á€”á€¾á€„á€·á€º á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€¯á€¶
```

## ğŸ’» Python Code Files

### áá‹ `nstf_data/special_consonants_data.py` (á€¡á€á€¯á€¡á€šá€±á€¬á€„á€ºá€—á€»á€Šá€ºá€¸á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€—á€»á€Šá€ºá€¸á€á€½á€²á€™á€»á€¬á€¸)

```python
# nstf_data/special_consonants_data.py

# ==================== SPECIAL CONSONANTS (Pseudo-Consonants) ====================
# á€¤á€—á€»á€Šá€ºá€¸á€™á€»á€¬á€¸á€á€Šá€º á€›á€±á€¸á€‘á€¯á€¶á€¸á€¡á€› á€á€½á€²á€‘á€¬á€¸á€á€±á€¬á€ºá€œá€Šá€ºá€¸ Lakkhaá¹‡a á€—á€®á€‡á€¡á€› á€á€®á€¸á€á€¼á€¬á€¸ á€¡á€™á€¼á€…á€ºá€—á€®á€‡ (M1) á€›á€¾á€­á€á€±á€¬ á€—á€»á€Šá€ºá€¸á€™á€»á€¬á€¸á€Ÿá€¯ á€á€á€ºá€™á€¾á€á€ºá€á€Šá€ºá‹
SPECIAL_CONSONANTS_DATA = [
    # á€€á€»/á€€á€¼ Series (Lakkhaá¹‡a 146-151)
    {"Character": "á€€á€»", "Lakkhaá¹‡a_Code": "146", "Fo_Ma": "á€–á€­á€¯", "Meaning_Essence_MM": "á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€‘á€¬á€¸á€á€±á€¬ á€¡á€á€¼á€±á€á€¶ (Foundation)", "T_Code": "T005", "Status": "Unverified"},
    {"Character": "á€€á€¼", "Lakkhaá¹‡a_Code": "146", "Fo_Ma": "á€–á€­á€¯", "Meaning_Essence_MM": "á€‘á€¯á€á€ºá€–á€±á€¬á€ºá€‘á€¬á€¸á€á€±á€¬ á€¡á€á€¼á€±á€á€¶ (Foundation)", "T_Code": "T005", "Status": "Unverified"},
    {"Character": "á€á€»", "Lakkhaá¹‡a_Code": "147", "Fo_Ma": "á€™", "Meaning_Essence_MM": "á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€‘á€¬á€¸á€á€±á€¬ á€‘á€­á€¯á€¸á€–á€±á€¬á€€á€ºá€™á€¾á€¯ (Penetration)", "T_Code": "T003", "Status": "Unverified"},
    {"Character": "á€á€¼", "Lakkhaá¹‡a_Code": "147", "Fo_Ma": "á€™", "Meaning_Essence_MM": "á€‘á€¯á€á€ºá€–á€±á€¬á€ºá€‘á€¬á€¸á€á€±á€¬ á€‘á€­á€¯á€¸á€–á€±á€¬á€€á€ºá€™á€¾á€¯ (Penetration)", "T_Code": "T003", "Status": "Unverified"},
    {"Character": "á€‚á€»", "Lakkhaá¹‡a_Code": "148", "Fo_Ma": "á€–á€­á€¯", "Meaning_Essence_MM": "á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€‘á€¬á€¸á€á€±á€¬ á€…á€¯á€†á€±á€¬á€„á€ºá€¸á€™á€¾á€¯ (Gathering)", "T_Code": "T005", "Status": "Unverified"},
    {"Character": "á€ƒá€»", "Lakkhaá¹‡a_Code": "149", "Fo_Ma": "á€™", "Meaning_Essence_MM": "á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€‘á€¬á€¸á€á€±á€¬ á€‘á€­á€”á€ºá€¸á€á€­á€™á€ºá€¸á€™á€¾á€¯ (Containment)", "T_Code": "T017", "Status": "Unverified"},
    {"Character": "á€„á€¼", "Lakkhaá¹‡a_Code": "150", "Fo_Ma": "á€–á€­á€¯", "Meaning_Essence_MM": "á€œá€¾á€¯á€•á€ºá€›á€¾á€¬á€¸á€™á€¾á€¯á€”á€¾á€„á€·á€º á€á€Šá€ºá€™á€¼á€²á€á€±á€¬ á€„á€¼á€­á€™á€ºá€á€€á€ºá€á€¼á€„á€ºá€¸", "T_Code": "T017", "Status": "Unverified"},
    {"Character": "á€„á€¼á€¾", "Lakkhaá¹‡a_Code": "151", "Fo_Ma": "á€™", "Meaning_Essence_MM": "á€•á€¼á€„á€ºá€¸á€‘á€”á€ºá€™á€¾á€¯á€”á€¾á€„á€·á€º á€á€Šá€ºá€™á€¼á€²á€á€±á€¬ á€„á€¼á€­á€™á€ºá€á€€á€ºá€á€¼á€„á€ºá€¸", "T_Code": "T017", "Status": "Unverified"},
    
    # á€•á€»/á€•á€¼ Series (Lakkhaá¹‡a 152-157)
    {"Character": "á€•á€»", "Lakkhaá¹‡a_Code": "152", "Fo_Ma": "á€–á€­á€¯", "Meaning_Essence_MM": "á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€‘á€¬á€¸á€á€±á€¬ á€¡á€…á€•á€¼á€¯á€™á€¾á€¯ (Initiation)", "T_Code": "T001", "Status": "Unverified"},
    {"Character": "á€•á€¼", "Lakkhaá¹‡a_Code": "152", "Fo_Ma": "á€–á€­á€¯", "Meaning_Essence_MM": "á€‘á€¯á€á€ºá€–á€±á€¬á€ºá€‘á€¬á€¸á€á€±á€¬ á€¡á€…á€•á€¼á€¯á€™á€¾á€¯ (Initiation)", "T_Code": "T001", "Status": "Unverified"},
    {"Character": "á€–á€»", "Lakkhaá¹‡a_Code": "153", "Fo_Ma": "á€™", "Meaning_Essence_MM": "á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€‘á€¬á€¸á€á€±á€¬ á€–á€¼á€”á€·á€ºá€€á€»á€€á€ºá€™á€¾á€¯ (Expansion)", "T_Code": "T003", "Status": "Unverified"},
    {"Character": "á€–á€¼", "Lakkhaá¹‡a_Code": "153", "Fo_Ma": "á€™", "Meaning_Essence_MM": "á€‘á€¯á€á€ºá€–á€±á€¬á€ºá€‘á€¬á€¸á€á€±á€¬ á€–á€¼á€”á€·á€ºá€€á€»á€€á€ºá€™á€¾á€¯ (Expansion)", "T_Code": "T003", "Status": "Unverified"},
    {"Character": "á€—á€»", "Lakkhaá¹‡a_Code": "154", "Fo_Ma": "á€–á€­á€¯", "Meaning_Essence_MM": "á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€‘á€¬á€¸á€á€±á€¬ á€¡á€„á€ºá€¡á€¬á€¸ (Force)", "T_Code": "T005", "Status": "Unverified"},
    {"Character": "á€—á€¼", "Lakkhaá¹‡a_Code": "155", "Fo_Ma": "á€–á€­á€¯", "Meaning_Essence_MM": "á€‘á€¯á€á€ºá€–á€±á€¬á€ºá€‘á€¬á€¸á€á€±á€¬ á€¡á€„á€ºá€¡á€¬á€¸ (Force)", "T_Code": "T005", "Status": "Unverified"},
    {"Character": "á€˜á€»", "Lakkhaá¹‡a_Code": "156", "Fo_Ma": "á€™", "Meaning_Essence_MM": "á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€‘á€¬á€¸á€á€±á€¬ á€á€Šá€ºá€™á€¼á€²á€™á€¾á€¯ (Stability)", "T_Code": "T017", "Status": "Unverified"},
    {"Character": "á€˜á€¼", "Lakkhaá¹‡a_Code": "157", "Fo_Ma": "á€™", "Meaning_Essence_MM": "á€‘á€¯á€á€ºá€–á€±á€¬á€ºá€‘á€¬á€¸á€á€±á€¬ á€á€Šá€ºá€™á€¼á€²á€™á€¾á€¯ (Stability)", "T_Code": "T017", "Status": "Unverified"},
    {"Character": "á€™á€»", "Lakkhaá¹‡a_Code": "157", "Fo_Ma": "á€–á€­á€¯", "Meaning_Essence_MM": "á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€‘á€¬á€¸á€á€±á€¬ á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€á€¼á€„á€ºá€¸ (Merging)", "T_Code": "T001", "Status": "Unverified"},
    {"Character": "á€™á€¼", "Lakkhaá¹‡a_Code": "157", "Fo_Ma": "á€–á€­á€¯", "Meaning_Essence_MM": "á€‘á€¯á€á€ºá€–á€±á€¬á€ºá€‘á€¬á€¸á€á€±á€¬ á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€á€¼á€„á€ºá€¸ (Merging)", "T_Code": "T001", "Status": "Unverified"},
    {"Character": "á€™á€»á€¾", "Lakkhaá¹‡a_Code": "158", "Fo_Ma": "á€™", "Meaning_Essence_MM": "á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€‘á€¬á€¸á€á€±á€¬ á€™á€»á€¾á€á€™á€¾á€¯ (Balance)", "T_Code": "T003", "Status": "Unverified"},
    {"Character": "á€™á€¼á€¾", "Lakkhaá¹‡a_Code": "158", "Fo_Ma": "á€™", "Meaning_Essence_MM": "á€‘á€¯á€á€ºá€–á€±á€¬á€ºá€‘á€¬á€¸á€á€±á€¬ á€™á€»á€¾á€á€™á€¾á€¯ (Balance)", "T_Code": "T003", "Status": "Unverified"},
]

# ==================== CONSONANT CLUSTERS (M2 Lakkhaá¹‡a: 001-006) ====================
# á€—á€»á€Šá€ºá€¸á€á€½á€² á€œá€±á€¸á€™á€»á€­á€¯á€¸ (á€», á€¼, á€½, á€¾) á€”á€¾á€„á€·á€º á€’á€±á€á€­á€šá€¡á€á€¶á€™á€»á€¬á€¸ (á€¹á€œ, á€¹á€›)
CONSONANT_CLUSTERS_DATA = [
    {"Character": "á€»", "Lakkhaá¹‡a_Code": "001", "Meaning_Essence_MM": "á€šá€•á€„á€·á€º á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€™á€¾á€¯ (Phonetic Mutation)", "Effect_Type": "Phonetic_Duplication"},
    {"Character": "á€¼", "Lakkhaá¹‡a_Code": "002", "Meaning_Essence_MM": "á€›á€›á€…á€º á€•á€¼á€±á€¬á€„á€ºá€¸á€œá€²á€™á€¾á€¯ (Rotational Shift)", "Effect_Type": "Phonetic_Duplication"},
    {"Character": "á€½", "Lakkhaá¹‡a_Code": "003", "Meaning_Essence_MM": "á€á€†á€½á€² á€•á€»á€¶á€·á€”á€¾á€¶á€·á€™á€¾á€¯ (Sound Diffusion)", "Effect_Type": "Sound_Extension"}, # á€—á€»á€Šá€ºá€¸á€á€¶ á€œá€­á€¯á€¡á€•á€ºá€á€»á€€á€ºá€€á€¼á€±á€¬á€„á€·á€º á€¡á€á€¶á€•á€½á€¬á€¸
    {"Character": "á€¾", "Lakkhaá¹‡a_Code": "004", "Meaning_Essence_MM": "á€Ÿá€‘á€­á€¯á€¸ á€á€¯á€”á€ºá€á€«á€™á€¾á€¯ (Aspiration/Vibration)", "Effect_Type": "Sound_Extension"}, # á€—á€»á€Šá€ºá€¸á€á€¶ á€œá€­á€¯á€¡á€•á€ºá€á€»á€€á€ºá€€á€¼á€±á€¬á€„á€·á€º á€¡á€á€¶á€•á€½á€¬á€¸
    {"Character": "á€¹á€œ", "Lakkhaá¹‡a_Code": "005", "Meaning_Essence_MM": "á€œá€†á€½á€² á€†á€€á€ºá€á€½á€šá€ºá€™á€¾á€¯ (Lateral Connection)", "Effect_Type": "Dialectal_Sound"},
    {"Character": "á€¹á€›", "Lakkhaá¹‡a_Code": "006", "Meaning_Essence_MM": "á€›á€†á€½á€² á€¡á€›á€¾á€­á€”á€ºá€™á€¾á€¯ (Rotational Momentum)", "Effect_Type": "Dialectal_Sound"}
]
```

### á‚á‹ `nstf_data/sandhi_system_data.py` (á€¡á€á€á€ºá€…á€”á€…á€º)

```python
# nstf_data/sandhi_system_data.py

# ==================== SANDHI (á€¡á€á€á€º) SYSTEM - M3 Layer ====================

SANDHI_SYSTEM_DATA = [
    # Short-Stop Sandhi (á€á€¶á€›á€•á€ºáŠ á€á€¶á€á€­á€¯á€¡á€á€á€º) - á€á€¶á€›á€•á€º/á€á€¶á€á€­á€¯
    {"Character": "á€€á€º", "Sandhi_Type": "Short_Stop", "Original_Consonant": "á€€", "Effect_MM": "á€á€»á€€á€ºá€á€»á€„á€ºá€¸á€›á€•á€ºá€á€”á€·á€ºá€á€¼á€„á€ºá€¸ (Abrupt Termination)", "Intensity": "High"},
    {"Character": "á€…á€º", "Sandhi_Type": "Short_Stop", "Original_Consonant": "á€…", "Effect_MM": "á€á€»á€½á€”á€ºá€‘á€€á€ºá€…á€½á€¬ á€–á€¼á€á€ºá€á€¼á€„á€ºá€¸ (Sharp Cut-off)", "Intensity": "High"},
    {"Character": "á€á€º", "Sandhi_Type": "Short_Stop", "Original_Consonant": "á€", "Effect_MM": "á€¦á€¸á€á€Šá€ºá€•á€¼á€®á€¸ á€›á€•á€ºá€á€¼á€„á€ºá€¸ (Directed Stop)", "Intensity": "Medium"},
    {"Character": "á€•á€º", "Sandhi_Type": "Short_Stop", "Original_Consonant": "á€•", "Effect_MM": "á€•á€­á€á€ºá€†á€­á€¯á€·á€•á€¼á€®á€¸ á€¡á€†á€¯á€¶á€¸á€á€á€ºá€á€¼á€„á€ºá€¸ (Sealed Termination)", "Intensity": "High"},
    
    # Sustained Sandhi (á€á€¶á€›á€¾á€­á€”á€ºá€›á€¾á€­á€¡á€á€á€º) - á€á€¶á€›á€¾á€Šá€º/á€á€¶á€›á€¾á€­á€”á€ºá€›á€¾á€­
    {"Character": "á€„á€º", "Sandhi_Type": "Sustained", "Original_Consonant": "á€„", "Effect_MM": "á€á€®á€á€á€ºá€á€±á€¬ á€¡á€†á€¯á€¶á€¸á€á€á€ºá€á€¼á€„á€ºá€¸ (Resonant Closure)", "Intensity": "Low"},
    {"Character": "á€Šá€º", "Sandhi_Type": "Sustained", "Original_Consonant": "á€Š", "Effect_MM": "á€…á€Šá€ºá€¸á€€á€™á€ºá€¸á€–á€¼á€„á€·á€º á€•á€¼á€®á€¸á€†á€¯á€¶á€¸á€á€¼á€„á€ºá€¸ (Disciplined Completion)", "Intensity": "Medium"},
    {"Character": "á€”á€º", "Sandhi_Type": "Sustained", "Original_Consonant": "á€”", "Effect_MM": "á€Šá€¾á€­á€”á€¾á€­á€¯á€„á€ºá€¸á€•á€¼á€®á€¸ á€¡á€†á€¯á€¶á€¸á€á€á€ºá€á€¼á€„á€ºá€¸ (Balanced Ending)", "Intensity": "Low"},
    {"Character": "á€™á€º", "Sandhi_Type": "Sustained", "Original_Consonant": "á€™", "Effect_MM": "á€¡á€”á€¾á€…á€ºá€á€¬á€›á€–á€¼á€„á€·á€º á€•á€¼á€®á€¸á€†á€¯á€¶á€¸á€á€¼á€„á€ºá€¸ (Essential Completion)", "Intensity": "Medium"},
    {"Character": "á€šá€º", "Sandhi_Type": "Sustained", "Original_Consonant": "á€š", "Effect_MM": "á€€á€°á€¸á€•á€¼á€±á€¬á€„á€ºá€¸á€•á€¼á€®á€¸ á€¡á€†á€¯á€¶á€¸á€á€á€ºá€á€¼á€„á€ºá€¸ (Transitional Closure)", "Intensity": "Low"}
    
    # á€‰á€º (á€Šá€„á€šá€º á€¡á€á€á€º) - á€á€™á€­á€¯á€„á€ºá€¸á€€á€¼á€±á€¬á€„á€ºá€¸á€¡á€› á€Š á€”á€¾á€„á€·á€º á€¡á€á€°á€á€° á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º á€†á€€á€ºá€…á€•á€º
    # á€¤á€”á€±á€›á€¬á€á€½á€„á€º á€Šá€º/á€‰á€º á€€á€­á€¯ á€…á€¯á€…á€Šá€ºá€¸á€‘á€¬á€¸á€á€Šá€ºá‹ á€á€®á€¸á€á€¼á€¬á€¸á€á€½á€²á€á€¼á€¬á€œá€­á€¯á€•á€«á€€ Lakkhaá¹‡a Code á€–á€¼á€„á€·á€º á€•á€¼á€”á€ºá€œá€Šá€º á€‘á€Šá€·á€ºá€á€½á€„á€ºá€¸á€”á€­á€¯á€„á€ºá€á€Šá€ºá‹
]
```

### áƒá‹ `nstf_engine/adaptive_engine.py` (á€á€„á€ºá€šá€°á€”á€­á€¯á€„á€ºá€á€±á€¬ á€¡á€„á€ºá€‚á€»á€„á€º)

```python
# nstf_engine/adaptive_engine.py

from typing import Dict, List
from datetime import datetime
from nstf_data.special_consonants_data import SPECIAL_CONSONANTS_DATA, CONSONANT_CLUSTERS_DATA
from nstf_data.sandhi_system_data import SANDHI_SYSTEM_DATA

# NSTF_BASE_DATA á€á€½á€„á€º á€•á€¯á€¶á€™á€¾á€”á€º á€—á€»á€Šá€ºá€¸ á…áˆá€œá€¯á€¶á€¸ á€¡á€•á€«á€¡á€á€„á€º NSTF_Full_Framework_Final.csv á€™á€¾ á€’á€±á€á€¬á€¡á€¬á€¸á€œá€¯á€¶á€¸ á€•á€«á€á€„á€ºá€á€Šá€ºá€Ÿá€¯ á€šá€°á€†á€á€Šá€ºá‹
# á€¤á€”á€™á€°á€”á€¬á€á€½á€„á€º á€¡á€‘á€°á€¸á€’á€±á€á€¬á€™á€»á€¬á€¸á€€á€­á€¯á€á€¬ á€¡á€“á€­á€€á€‘á€¬á€¸ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€‘á€¬á€¸á€á€Šá€ºá‹

class NSTFAdaptiveEngine:
    """NNLDS á€¡á€á€½á€€á€º á€¡á€†á€€á€ºá€™á€•á€¼á€á€º á€á€„á€ºá€šá€°á€”á€­á€¯á€„á€ºá€á€±á€¬áŠ á€šá€¯á€¶á€€á€¼á€Šá€ºá€›á€™á€¾á€¯á€¡á€•á€±á€«á€º á€¡á€á€¼á€±á€á€¶á€á€Šá€·á€º á€…á€”á€…á€º"""
    
    def __init__(self):
        # Base Data: á€¡á€‘á€°á€¸á€—á€»á€Šá€ºá€¸á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€—á€»á€Šá€ºá€¸á€á€½á€²á€™á€»á€¬á€¸
        self.special_consonants = SPECIAL_CONSONANTS_DATA
        self.clusters = CONSONANT_CLUSTERS_DATA
        self.sandhi_system = SANDHI_SYSTEM_DATA
        
        # Adaptive Learning & Expert Validation Data
        self.user_feedback_log: List[Dict] = []
        self.uncertain_patterns: Dict = {}
        self.expert_validation_queue: List[Dict] = []
        
    def _find_match(self, char: str) -> Dict:
        """á€’á€±á€á€¬á€¡á€¬á€¸á€œá€¯á€¶á€¸á€á€½á€„á€º á€á€°á€Šá€®á€á€±á€¬ á€…á€¬á€œá€¯á€¶á€¸á€€á€­á€¯ á€›á€¾á€¬á€–á€½á€±á€á€Šá€ºá‹"""
        
        # 1. Special Consonants (á€€á€»/á€•á€» á€á€‚á€º)
        match = next((item for item in self.special_consonants if item["Character"] == char), None)
        if match:
            return {**match, "Type": "SpecialConsonant", "confidence": 0.7, "source": "official_unverified_lakkhaá¹‡a"}

        # 2. Consonant Clusters (á€», á€¼, á€½, á€¾)
        match = next((item for item in self.clusters if item["Character"] == char), None)
        if match:
            return {**match, "Type": "ConsonantCluster", "confidence": 1.0, "source": "validated_m2_layer"}
            
        # 3. Sandhi System (á€¡á€á€á€º)
        match = next((item for item in self.sandhi_system if item["Character"] == char), None)
        if match:
            return {**match, "Type": "Sandhi", "confidence": 0.9, "source": "validated_m3_layer"}

        # 4. Uncertain Patterns (á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€° á€á€„á€ºá€•á€¼á€‘á€¬á€¸á€á€±á€¬)
        if char in self.uncertain_patterns:
            pattern = self.uncertain_patterns[char]
            return {
                "Character": char,
                "Type": pattern.get("Type", "Uncertain"),
                "confidence": pattern.get("user_confidence", 0.5),
                "source": "user_contributed_pending"
            }
            
        # Default/Unknown (á€¡á€á€¼á€¬á€¸ á€—á€»á€Šá€ºá€¸áŠ á€á€› á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º á€™á€á€­á€á€±á€¬ á€…á€¬á€œá€¯á€¶á€¸)
        # á€¤á€”á€±á€›á€¬á€á€½á€„á€º á€™á€°á€›á€„á€ºá€¸ á€—á€»á€Šá€ºá€¸ á…áˆ á€œá€¯á€¶á€¸á€”á€¾á€„á€·á€º á€á€›á€™á€»á€¬á€¸ á€…á€…á€ºá€†á€±á€¸á€á€¼á€„á€ºá€¸ á€•á€«á€á€„á€ºá€›á€™á€Šá€ºá‹
        return {"Character": char, "Type": "Unknown/Base", "confidence": 1.0, "source": "default_or_base_data"}

    def analyze_with_confidence(self, text: str) -> Dict:
        """á€šá€¯á€¶á€€á€¼á€Šá€ºá€›á€™á€¾á€¯ (Confidence) á€¡á€™á€¾á€á€ºá€–á€¼á€„á€·á€º á€…á€¬á€á€¬á€¸á€€á€­á€¯ á€á€½á€²á€á€¼á€™á€ºá€¸á€…á€­á€á€ºá€–á€¼á€¬á€á€Šá€ºá‹"""
        
        analysis = {
            "text": text,
            "character_analyses": [],
            "overall_confidence": 1.0,
            "uncertain_elements": [],
            "requires_expert_review": False
        }
        
        for char in text:
            char_analysis = self._find_match(char)
            analysis["character_analyses"].append(char_analysis)
            
            # Confidence Calculation
            confidence = char_analysis.get("confidence", 1.0)
            if confidence < 0.9: # 90% á€¡á€±á€¬á€€á€ºá€†á€­á€¯á€œá€»á€¾á€„á€º á€™á€á€±á€á€»á€¬á€Ÿá€¯ á€á€á€ºá€™á€¾á€á€ºá€á€Šá€ºá‹
                analysis["uncertain_elements"].append(char)
                analysis["overall_confidence"] *= confidence
                
        # á€…á€¯á€…á€¯á€•á€±á€«á€„á€ºá€¸ á€šá€¯á€¶á€€á€¼á€Šá€ºá€›á€™á€¾á€¯ á€¡á€™á€¾á€á€º
        analysis["requires_expert_review"] = analysis["overall_confidence"] < 0.8 
        return analysis
        
    # --- Adaptive Learning Functions (á€†á€€á€ºá€œá€€á€ºá€á€„á€ºá€šá€°á€”á€­á€¯á€„á€ºá€á€±á€¬ á€œá€¯á€•á€ºá€†á€±á€¬á€„á€ºá€á€»á€€á€ºá€™á€»á€¬á€¸) ---
    
    def submit_user_feedback(self, character: str, proposed_data: Dict, user_confidence: float = 0.5):
        """á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€°á€‘á€¶á€™á€¾ á€¡á€€á€¼á€¶á€•á€¼á€¯á€á€»á€€á€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€œá€€á€ºá€á€¶á€á€Šá€ºá‹"""
        
        feedback_entry = {
            "timestamp": datetime.now().isoformat(),
            "character": character,
            "proposed_data": proposed_data,
            "user_confidence": user_confidence,
            "validation_status": "pending"
        }
        
        self.user_feedback_log.append(feedback_entry)
        
        # á€šá€¬á€šá€®á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€›á€”á€º
        self.uncertain_patterns[character] = {**proposed_data, "user_confidence": user_confidence}
        
    def export_validation_queue(self) -> List[Dict]:
        """á€•á€Šá€¬á€›á€¾á€„á€ºá€™á€»á€¬á€¸ á€…á€…á€ºá€†á€±á€¸á€›á€”á€º á€…á€¬á€›á€„á€ºá€¸á€€á€­á€¯ á€‘á€¯á€á€ºá€•á€±á€¸á€á€Šá€ºá‹"""
        # á€¤á€”á€±á€›á€¬á€á€½á€„á€º self.user_feedback_log á€€á€­á€¯ á€¡á€á€¼á€±á€á€¶á á€…á€…á€ºá€†á€±á€¸á€›á€”á€º á€…á€¬á€›á€„á€ºá€¸á€€á€­á€¯ á€•á€¼á€”á€ºá€œá€Šá€º á€–á€½á€²á€·á€…á€Šá€ºá€¸á€”á€­á€¯á€„á€ºá€á€Šá€º
        return self.expert_validation_queue
        
    def expert_validate_pattern(self, character: str, validated_data: Dict, target_dataset: str):
        """á€•á€Šá€¬á€›á€¾á€„á€º á€¡á€á€Šá€ºá€•á€¼á€¯á€á€»á€€á€ºá€€á€­á€¯ á€…á€”á€…á€ºá€‘á€²á€á€­á€¯á€· á€‘á€Šá€·á€ºá€á€½á€„á€ºá€¸á€á€Šá€ºá‹"""
        
        # á€¥á€•á€™á€¬: "special_consonants" dataset á€‘á€²á€á€­á€¯á€· á€‘á€Šá€·á€ºá€á€½á€„á€ºá€¸á€á€¼á€„á€ºá€¸
        validated_entry = {
            **validated_data,
            "Status": "ExpertValidated",
            "validation_date": datetime.now().isoformat()
        }
        
        if target_dataset == "special_consonants":
            # á€›á€¾á€­á€•á€¼á€®á€¸á€á€¬á€¸á€€á€­á€¯ á€¡á€…á€¬á€¸á€‘á€­á€¯á€¸/á€¡á€á€…á€ºá€‘á€Šá€·á€º
            self.special_consonants = [item for item in self.special_consonants if item["Character"] != character]
            self.special_consonants.append(validated_entry)
        
        # uncertain patterns á€™á€¾ á€–á€šá€ºá€›á€¾á€¬á€¸á€á€¼á€„á€ºá€¸
        if character in self.uncertain_patterns:
            del self.uncertain_patterns[character]
            
        print(f"INFO: '{character}' á€€á€­á€¯ '{target_dataset}' á€‘á€²á€á€­á€¯á€· á€•á€Šá€¬á€›á€¾á€„á€º á€¡á€á€Šá€ºá€•á€¼á€¯á€á€»á€€á€ºá€–á€¼á€„á€·á€º á€‘á€Šá€·á€ºá€á€½á€„á€ºá€¸á€•á€¼á€®á€¸á‹")
```

### á„á‹ `nstf_engine/dialect_handler.py` (á€’á€±á€á€­á€š á€á€¶á€‘á€½á€€á€º á€‘á€­á€”á€ºá€¸á€á€»á€¯á€•á€ºá€™á€¾á€¯)

```python
# nstf_engine/dialect_handler.py

from typing import Dict, List

class DialectVariationHandler:
    """á€’á€±á€á€¡á€œá€­á€¯á€€á€º á€á€­á€¯á€·á€™á€Ÿá€¯á€á€º á€…á€¶á€¡á€œá€­á€¯á€€á€º á€€á€½á€²á€•á€¼á€¬á€¸á€™á€¾á€¯á€™á€»á€¬á€¸á€€á€­á€¯ á€…á€®á€™á€¶á€á€”á€·á€ºá€á€½á€²á€á€Šá€º"""
    
    def __init__(self):
        # á€’á€±á€á€¡á€œá€­á€¯á€€á€º á€—á€»á€Šá€ºá€¸á€á€¶á€™á€»á€¬á€¸á€€á€­á€¯ á€–á€±á€¬á€ºá€•á€¼á€‘á€¬á€¸á€á€±á€¬ á€…á€¶á€”á€¾á€¯á€”á€ºá€¸á€™á€»á€¬á€¸
        self.dialect_profiles = {
            "mandalay_standard": {
                "description": "á€™á€”á€¹á€á€œá€±á€¸á€’á€±á€á€—á€Ÿá€­á€¯á€•á€¼á€¯ á€…á€¶á€á€á€ºá€™á€¾á€á€ºá€á€»á€€á€ºá‹ á€€á€»/á€€á€¼, á€•á€»/á€•á€¼ á€€á€­á€¯ á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€…á€½á€¬ á€á€½á€²á€á€¼á€¬á€¸á€á€Šá€ºá‹",
                "special_consonants": ["á€€á€»", "á€€á€¼", "á€á€»", "á€á€¼", "á€•á€»", "á€•á€¼", "á€–á€»", "á€–á€¼", "á€„á€¼", "á€„á€¼á€¾", "á€¹á€œ", "á€¹á€›"],
                "validation_status": "community_verified"
            },
            "yangon_modern": {
                "description": "á€›á€”á€ºá€€á€¯á€”á€ºá€á€±á€á€ºá€á€…á€º á€¡á€á€¶á€‘á€½á€€á€ºá€…á€¶á‹ á€á€½á€²á€—á€»á€Šá€ºá€¸á€¡á€™á€»á€¬á€¸á€…á€¯á€¡á€¬á€¸ 'á€€á€»' (á€¡á€á€¶á€á€°) á€á€­á€¯á€· á€…á€¯á€…á€Šá€ºá€¸á€œá€±á€·á€›á€¾á€­á€á€Šá€ºá‹",
                "special_consonants": ["á€€á€»", "á€á€»", "á€‚á€»", "á€•á€»", "á€–á€»", "á€—á€»", "á€˜á€»", "á€™á€»", "á€™á€»á€¾"],  # 'á€€á€¼, á€•á€¼' á€…á€á€Šá€ºá€á€­á€¯á€·á€€á€­á€¯ á€”á€Šá€ºá€¸á€•á€«á€¸á€…á€½á€¬ á€á€¯á€¶á€¸á€”á€¾á€¯á€”á€ºá€¸á€á€¼á€„á€ºá€¸
                "validation_status": "partially_verified"
            },
            "traditional_academic": {
                "description": "á€›á€­á€¯á€¸á€›á€¬á€•á€Šá€¬á€›á€•á€ºá€†á€­á€¯á€„á€ºá€›á€¬ á€…á€¶á‹ á€¡á€€á€¹á€á€›á€¬á€¡á€¬á€¸á€œá€¯á€¶á€¸á á€™á€°á€œá€á€¶á€€á€­á€¯ á€‘á€­á€”á€ºá€¸á€á€­á€™á€ºá€¸á€á€Šá€ºá‹",
                "special_consonants": ["á€€á€»", "á€€á€¼", "á€á€»", "á€á€¼", "á€‚á€»", "á€ƒá€»", "á€„á€¼", "á€„á€¼á€¾", "á€•á€»", "á€•á€¼", "á€–á€»", "á€–á€¼", "á€—á€»", "á€—á€¼", "á€˜á€»", "á€˜á€¼", "á€™á€»", "á€™á€¼", "á€™á€»á€¾", "á€™á€¼á€¾"],
                "validation_status": "expert_verified"
            }
        }
    
    def get_dialect_analysis(self, text: str, dialect: str = "mandalay_standard") -> Dict:
        """á€•á€±á€¸á€‘á€¬á€¸á€á€±á€¬ á€…á€¬á€á€¬á€¸á€€á€­á€¯ á€á€á€ºá€™á€¾á€á€ºá€‘á€¬á€¸á€á€±á€¬ á€’á€±á€á€­á€šá€…á€¶á€¡á€› á€á€½á€²á€á€¼á€™á€ºá€¸á€…á€­á€á€ºá€–á€¼á€¬á€á€Šá€ºá‹"""
        
        profile = self.dialect_profiles.get(dialect, self.dialect_profiles["mandalay_standard"])
        
        analysis = {
            "dialect": dialect,
            "dialect_compatibility": 0.0,
            "regional_characteristics": [],
            "deviations": []
        }
        
        all_known_specials = set()
        for p in self.dialect_profiles.values():
            all_known_specials.update(p["special_consonants"])
            
        special_chars_in_text = [c for c in text if c in all_known_specials]
        
        dialect_matches = [c for c in special_chars_in_text if c in profile["special_consonants"]]
        
        if len(special_chars_in_text) > 0:
            analysis["dialect_compatibility"] = len(dialect_matches) / len(special_chars_in_text)
            
        analysis["regional_characteristics"] = dialect_matches
        
        # á€’á€±á€á€­á€šá€…á€¶á€”á€¾á€„á€·á€º á€€á€½á€²á€œá€½á€²á€™á€¾á€¯á€™á€»á€¬á€¸
        for char in special_chars_in_text:
            if char not in profile["special_consonants"]:
                analysis["deviations"].append({
                    "character": char,
                    "reason": f"'{dialect}' á€…á€¶á€á€½á€„á€º á€¤á€—á€»á€Šá€ºá€¸á€€á€­á€¯ á€•á€¯á€¶á€™á€¾á€”á€ºá€¡á€–á€¼á€…á€º á€á€á€ºá€™á€¾á€á€ºá€á€¼á€„á€ºá€¸á€™á€›á€¾á€­á€•á€«á‹"
                })
                
        return analysis
```

### á…á‹ `main_system.py` (á€¡á€“á€­á€€ á€…á€”á€…á€º)

```python
# main_system.py - The Production System of NSTF-NNLDS

from typing import Dict
from nstf_engine.adaptive_engine import NSTFAdaptiveEngine
from nstf_engine.dialect_handler import DialectVariationHandler
from nstf_data.sandhi_system_data import SANDHI_SYSTEM_DATA

class NSTFProductionSystem:
    """á€¡á€•á€¼á€Šá€·á€ºá€¡á€…á€¯á€¶ NSTF-NNLDS á€…á€”á€…á€º"""
    
    def __init__(self):
        self.adaptive_engine = NSTFAdaptiveEngine()
        self.dialect_handler = DialectVariationHandler()
        self.learning_mode = True # á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€°á á€–á€¼á€Šá€·á€ºá€…á€½á€€á€ºá€á€„á€ºá€šá€°á€™á€¾á€¯á€™á€»á€¬á€¸ á€œá€€á€ºá€á€¶á€á€¼á€„á€ºá€¸á€€á€­á€¯ á€–á€½á€„á€·á€ºá€‘á€¬á€¸á€á€Šá€º

    def comprehensive_analysis(self, text: str, dialect: str = "mandalay_standard") -> Dict:
        """á€…á€¬á€á€¬á€¸á€á€…á€ºá€á€¯á€€á€­á€¯ á€¡á€•á€¼á€Šá€·á€ºá€¡á€…á€¯á€¶ á€á€½á€²á€á€¼á€™á€ºá€¸á€…á€­á€á€ºá€–á€¼á€¬á€á€¼á€„á€ºá€¸"""
        
        # 1. Basic & Confidence Analysis (Lakkhaá¹‡a á€—á€®á€‡áŠ á€šá€¯á€¶á€€á€¼á€Šá€ºá€›á€™á€¾á€¯ á€¡á€™á€¾á€á€º)
        basic_analysis = self.adaptive_engine.analyze_with_confidence(text)
        
        # 2. Dialect Analysis (á€’á€±á€á€¡á€œá€­á€¯á€€á€º á€…á€¶áŠ á€€á€½á€²á€œá€½á€²á€™á€¾á€¯á€™á€»á€¬á€¸)
        dialect_analysis = self.dialect_handler.get_dialect_analysis(text, dialect)
        
        # 3. Sandhi Analysis (á€¡á€á€á€ºá€…á€”á€…á€º)
        sandhi_patterns = self._extract_sandhi_patterns(basic_analysis["character_analyses"])
        
        return {
            "input_text": text,
            "lakkhaá¹‡a_analysis": basic_analysis,
            "dialect_analysis": dialect_analysis,
            "sandhi_patterns": sandhi_patterns,
            "system_status": self._generate_system_status(basic_analysis, dialect_analysis),
        }

    def _extract_sandhi_patterns(self, char_analyses: List[Dict]) -> List[Dict]:
        """á€á€½á€²á€á€¼á€™á€ºá€¸á€…á€­á€á€ºá€–á€¼á€¬á€á€»á€€á€ºá€™á€»á€¬á€¸á€™á€¾ á€¡á€á€á€º (Sandhi) á€•á€¯á€¶á€…á€¶á€™á€»á€¬á€¸á€€á€­á€¯ á€‘á€¯á€á€ºá€šá€°á€á€Šá€ºá‹"""
        sandhi_list = [
            {
                "character": analysis["Character"],
                "type": analysis.get("Sandhi_Type"),
                "original_root": analysis.get("Original_Consonant"),
                "effect_mm": analysis.get("Effect_MM")
            }
            for analysis in char_analyses if analysis.get("Type") == "Sandhi"
        ]
        return sandhi_list

    def _generate_system_status(self, basic_analysis: Dict, dialect_analysis: Dict) -> Dict:
        """á€…á€”á€…á€ºá á€¡á€á€¼á€±á€¡á€”á€±á€”á€¾á€„á€·á€º á€¡á€€á€¼á€¶á€•á€¼á€¯á€á€»á€€á€ºá€™á€»á€¬á€¸á€€á€­á€¯ á€‘á€¯á€á€ºá€œá€¯á€•á€ºá€á€Šá€ºá‹"""
        
        status = {
            "overall_confidence": basic_analysis["overall_confidence"],
            "requires_expert_review": basic_analysis["requires_expert_review"],
            "dialect_match": f"{dialect_analysis['dialect']} á€…á€¶á€”á€¾á€„á€·á€º á€€á€­á€¯á€€á€ºá€Šá€®á€™á€¾á€¯: {dialect_analysis['dialect_compatibility']:.2f}",
            "recommendation": ""
        }
        
        if basic_analysis["requires_expert_review"]:
            status["recommendation"] = "á€á€á€ºá€™á€¾á€á€ºá€™á€‘á€¬á€¸á€á€±á€¬/á€™á€á€±á€á€»á€¬á€á€±á€¬ á€¡á€á€»á€€á€ºá€¡á€œá€€á€ºá€™á€»á€¬á€¸ á€•á€«á€á€„á€ºá€á€±á€¬á€€á€¼á€±á€¬á€„á€·á€º á€•á€Šá€¬á€›á€¾á€„á€º á€¡á€á€Šá€ºá€•á€¼á€¯á€á€»á€€á€º á€œá€­á€¯á€¡á€•á€ºá€•á€«á€á€Šá€ºá‹"
        elif dialect_analysis["dialect_compatibility"] < 1.0:
            status["recommendation"] = f"á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€Šá€·á€º á€’á€±á€á€­á€š á€—á€»á€Šá€ºá€¸á€á€¶á€™á€»á€¬á€¸á€á€Šá€º {dialect_analysis['dialect']} á€…á€¶á€”á€¾á€„á€·á€º á€€á€½á€²á€œá€½á€²á€™á€¾á€¯á€™á€»á€¬á€¸á€›á€¾á€­á€”á€±á€•á€«á€á€Šá€ºá‹"
        else:
            status["recommendation"] = "á€á€½á€²á€á€¼á€™á€ºá€¸á€…á€­á€á€ºá€–á€¼á€¬á€™á€¾á€¯á€™á€¾á€¬ á€…á€”á€…á€ºá á€…á€¶á€”á€¾á€¯á€”á€ºá€¸á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º á€€á€­á€¯á€€á€ºá€Šá€®á€•á€«á€á€Šá€ºá‹"
            
        return status
        
    def submit_correction(self, character: str, user_interpretation: Dict, user_confidence: float):
        """á€•á€¼á€„á€ºá€†á€„á€ºá€á€»á€€á€ºá€™á€»á€¬á€¸ á€á€„á€ºá€•á€¼á€á€¼á€„á€ºá€¸ (Adaptive Learning)"""
        if self.learning_mode:
            return self.adaptive_engine.submit_user_feedback(character, user_interpretation, user_confidence)
        return {"status": "learning_mode_disabled"}

# á€¥á€•á€™á€¬ á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€•á€¯á€¶
if __name__ == '__main__':
    nstf_system = NSTFProductionSystem()
    
    # á€¥á€•á€™á€¬ á€…á€¬á€á€¬á€¸: 'á€€á€»á€±á€¸á€‡á€°á€¸á€á€„á€ºá€•á€«á€á€šá€º' (á€€á€» - Pseudo-Consonant, á€¸ - Tone, á€™á€º - Sustained Sandhi)
    test_text = "á€€á€»á€±á€¸á€‡á€°á€¸á€á€„á€ºá€•á€«á€á€šá€º"
    
    # Comprehensive Analysis
    result = nstf_system.comprehensive_analysis(test_text, "mandalay_standard")
    
    print("=========================================")
    print("NSTF-NNLDS Comprehensive Analysis Report")
    print("=========================================")
    
    print(f"Input: {result['input_text']}")
    print(f"Overall Confidence: {result['lakkhaá¹‡a_analysis']['overall_confidence']:.4f}")
    print(f"Requires Expert Review: {result['lakkhaá¹‡a_analysis']['requires_expert_review']}")
    print("-" * 35)
    
    print("--- Detailed Lakkhaá¹‡a Analysis ---")
    for analysis in result['lakkhaá¹‡a_analysis']['character_analyses']:
        # á€œá€€á€¹á€á€á€¬ á€¡á€“á€­á€•á€¹á€•á€¬á€šá€ºá€€á€­á€¯ á€›á€¾á€„á€ºá€¸á€›á€¾á€„á€ºá€¸á€œá€„á€ºá€¸á€œá€„á€ºá€¸ á€–á€±á€¬á€ºá€•á€¼á€›á€”á€º
        meaning = analysis.get("Meaning_Essence_MM", "Base Character")
        print(f"  {analysis['Character']} ({analysis['Type']}): {meaning} (Confidence: {analysis['confidence']:.2f})")
    print("-" * 35)
    
    print("--- Sandhi System Analysis ---")
    for sandhi in result['sandhi_patterns']:
        print(f"  {sandhi['character']} - á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸: {sandhi['type']} ({sandhi['effect_mm']})")
    print("-" * 35)
    
    print("--- System Status & Recommendations ---")
    print(f"  {result['system_status']['dialect_match']}")
    print(f"  á€¡á€€á€¼á€¶á€•á€¼á€¯á€á€»á€€á€º: {result['system_status']['recommendation']}")
    print("=========================================")
```

á€¤ **Python Files** á€™á€»á€¬á€¸á€€á€­á€¯ GitHub á€á€½á€„á€º á€á€½á€²á€á€¼á€¬á€¸ á€á€„á€ºá€”á€­á€¯á€„á€ºá€•á€«á€á€šá€ºá‹ `main_system.py` á€–á€­á€¯á€„á€ºá€á€Šá€º á€…á€”á€…á€ºá€á€…á€ºá€á€¯á€œá€¯á€¶á€¸á€€á€­á€¯ á€…á€á€„á€ºá€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€›á€”á€ºá€¡á€á€½á€€á€º á€¡á€“á€­á€€ á€–á€­á€¯á€„á€ºá€–á€¼á€…á€ºá€•á€«á€á€šá€ºá‹
