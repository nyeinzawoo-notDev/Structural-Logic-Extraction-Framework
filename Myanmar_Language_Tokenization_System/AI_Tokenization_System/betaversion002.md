á€Ÿá€¯á€á€ºá€€á€²á€·áŠ á€á€„á€·á€ºá€›á€²á€· NNLDSMyanmarTokenizer Class á€€á€­á€¯ Dynamic Semantic Root Discovery á€…á€”á€…á€ºá€”á€²á€·á€¡á€á€° á€•á€¼á€„á€ºá€†á€„á€ºá€•á€±á€¸á€•á€«á€™á€šá€ºá‹

```python
"""
NNLDS Myanmar Tokenization Engine - Complete Data Integration
GitHub Ready Version with Full C93 + V73 Dataset
WITH DYNAMIC SEMANTIC ROOT DISCOVERY SYSTEM
"""

import re
import json
import random
from typing import Dict, List, Tuple, Any, Optional
from datetime import datetime

class NNLDSMyanmarTokenizer:
    """
    NNLDS Myanmar Tokenization Engine - Complete Implementation
    Now with FULL 93 Consonants + 73 Vowels dataset + DYNAMIC SEMANTIC DISCOVERY
    """
    
    def __init__(self, cons_data: Dict = None, vowel_data: Dict = None, 
                 coupling_data: Dict = None, protocol_data: Dict = None):
        """
        Initialize the NNLDS Tokenizer with complete dataset.
        """
        # 1. Modular Data Loading with FULL dataset
        self.consonants = cons_data if cons_data else self._load_complete_consonant_data()
        self.vowels = vowel_data if vowel_data else self._load_complete_vowel_data()
        self.semantic_couplings = coupling_data if coupling_data else self._load_coupling_data()
        self.master_protocol = protocol_data if protocol_data else self._load_protocol_data()

        # 2. Data Integrity Verification
        self._check_data_integrity()
        
        # 3. DYNAMIC SEMANTIC ROOT DISCOVERY SYSTEM
        self.discover_semantic_roots()
        
        # 4. Validation System
        self.validation_queue = []
        
        # 5. Myanmar Unicode Patterns
        self._init_myanmar_unicode_patterns()
        
        # 6. System Initialization
        self._print_system_introduction()

    # =========================================================================
    # ğŸ“š COMPLETE DATA LOADING METHODS WITH PENDING DISCOVERY
    # =========================================================================

    def _load_complete_consonant_data(self) -> Dict:
        """Load COMPLETE C1-C93 consonant data with PENDING_DISCOVERY semantic roots."""
        consonants = {}
        
        # Complete consonant data from provided table - ONLY 10 SEEDS WITH DEFINED SEMANTIC ROOTS
        consonant_data = [
            # C.no, á€—á€»á€Šá€ºá€¸, á€¡á€€á€¹á€á€›á€¬Code, á€¡á€™á€»á€­á€¯á€¸á€¡á€…á€¬á€¸, á€™á€­á€á€ºá€˜á€€á€ºá€—á€»á€Šá€ºá€¸, á€á€¯á€’á€¹á€“/á€€á€¬á€›á€­á€¯á€€á€º-á€—á€»á€Šá€ºá€¸Code-á€—á€»á€Šá€ºá€¸á€á€½á€²Code-á€á€›Code-á€¡á€€á€¹á€á€›á€¬
            ('C01', 'á€€', '101', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '102', 'á€á-á€–á€­á€¯-101-0-201-á€€', 'ACTION_BASE', 'Action/Base Form'),
            ('C02', 'á€', '102', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '101', 'á€á‚-á€™-102-0-201-á€', 'FORCE_ENERGY', 'Force/Energy'),
            ('C03', 'á€‚', '103', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '103', 'á€áƒ-á€–á€­á€¯-103-0-201-á€‚', 'CONTAINMENT', 'Container/Holder'),
            ('C04', 'á€ƒ', '104', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '102', 'á€á„-á€™-104-0-201-á€ƒ', 'INTENSITY', 'Intensity'),
            ('C05', 'á€„', '105', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '106', 'á€á…-á€–á€­á€¯-105-0-201-á€„', 'RESONANCE', 'Resonance/Echo'),
            ('C06', 'á€„á€¾', '106', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '104', 'á€á†-á€™-106-0-201-á€„á€¾', 'RESONANCE_MODIFIED', 'Modified Resonance'),
            ('C07', 'á€…', '107', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '106', 'á€á‡-á€–á€­á€¯-107-0-201-á€…', 'MECHANISM_TOOL', 'Machine/Tool'),
            ('C08', 'á€†', '108', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '106', 'á€áˆ-á€™-108-0-201-á€†', 'MECHANISM_ENERGIZED', 'Energized Mechanism'),
            ('C09', 'á€‡', '109', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '106', 'á€á‰-á€–á€­á€¯-109-0-201-á€‡', 'CONTAINER_REFINED', 'Refined Container'),
            ('C10', 'á€ˆ', '110', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '106', 'áá€-á€™-110-0-201-á€ˆ', 'CONTAINER_INTENSIFIED', 'Intensified Container'),
            ('C11', 'á€Š', '111', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '106', 'áá-á€–á€­á€¯-111-0-201-á€Š', 'PALATAL_BASE', 'PENDING_DISCOVERY'),
            ('C12', 'á€Šá€¾', '112', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '106', 'áá‚-á€™-112-0-201-á€Šá€¾', 'PALATAL_MODIFIED', 'PENDING_DISCOVERY'),
            ('C13', 'á€‹', '113', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '106', 'ááƒ-á€–á€­á€¯-113-0-201-á€‹', 'RETROFLEX_BASE', 'PENDING_DISCOVERY'),
            ('C14', 'á€Œ', '114', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '106', 'áá„-á€™-114-0-201-á€Œ', 'RETROFLEX_ENERGY', 'PENDING_DISCOVERY'),
            ('C15', 'á€', '115', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '106', 'áá…-á€–á€­á€¯-115-0-201-á€', 'RETROFLEX_CONTAINER', 'PENDING_DISCOVERY'),
            ('C16', 'á€', '116', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '106', 'áá†-á€™-116-0-201-á€', 'RETROFLEX_INTENSITY', 'PENDING_DISCOVERY'),
            ('C17', 'á€', '117', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '106', 'áá‡-á€–á€­á€¯-117-0-201-á€', 'CEREBRAL_BASE', 'PENDING_DISCOVERY'),
            ('C18', 'á€á€¾', '118', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '106', 'ááˆ-á€™-118-0-201-á€á€¾', 'CEREBRAL_MODIFIED', 'PENDING_DISCOVERY'),
            ('C19', 'á€', '119', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '106', 'áá‰-á€–á€­á€¯-119-0-201-á€', 'DENTAL_BASE', 'PENDING_DISCOVERY'),
            ('C20', 'á€‘', '120', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '106', 'á‚á€-á€™-120-0-201-á€‘', 'DENTAL_ENERGY', 'PENDING_DISCOVERY'),
            ('C21', 'á€’', '121', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '106', 'á‚á-á€–á€­á€¯-121-0-201-á€’', 'DENTAL_CONTAINER', 'PENDING_DISCOVERY'),
            ('C22', 'á€“', '122', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '106', 'á‚á‚-á€™-122-0-201-á€“', 'DENTAL_INTENSITY', 'PENDING_DISCOVERY'),
            ('C23', 'á€”', '123', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '106', 'á‚áƒ-á€–á€­á€¯-123-0-201-á€”', 'ALVEOLAR_BASE', 'PENDING_DISCOVERY'),
            ('C24', 'á€”á€¾', '124', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '106', 'á‚á„-á€™-124-0-201-á€”á€¾', 'ALVEOLAR_MODIFIED', 'PENDING_DISCOVERY'),
            ('C25', 'á€•', '125', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '125', 'á‚á…-á€–á€­á€¯-125-0-201-á€•', 'LABIAL_BASE', 'PENDING_DISCOVERY'),
            ('C26', 'á€–', '126', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '106', 'á‚á†-á€™-126-0-201-á€–', 'LABIAL_ENERGY', 'PENDING_DISCOVERY'),
            ('C27', 'á€—', '127', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '127', 'á‚á‡-á€–á€­á€¯-127-0-201-á€—', 'LABIAL_CONTAINER', 'PENDING_DISCOVERY'),
            ('C28', 'á€˜', '128', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '126', 'á‚áˆ-á€™-128-0-201-á€˜', 'LABIAL_INTENSITY', 'PENDING_DISCOVERY'),
            ('C29', 'á€™', '129', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '130', 'á‚á‰-á€–á€­á€¯-129-0-201-á€™', 'BILABIAL_BASE', 'PENDING_DISCOVERY'),
            ('C30', 'á€™á€¾', '130', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '128', 'áƒá€-á€™-130-0-201-á€™á€¾', 'BILABIAL_MODIFIED', 'PENDING_DISCOVERY'),
            ('C31', 'á€š', '131', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '130', 'áƒá-á€–á€­á€¯-131-0-201-á€š', 'PALATAL_GLIDE', 'PENDING_DISCOVERY'),
            ('C32', 'á€šá€¾', '132', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '130', 'áƒá‚-á€™-132-0-201-á€šá€¾', 'PALATAL_GLIDE_MODIFIED', 'PENDING_DISCOVERY'),
            ('C33', 'á€›', '133', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '130', 'áƒáƒ-á€–á€­á€¯-133-0-201-á€›', 'ALVEOLAR_APPROXIMANT', 'PENDING_DISCOVERY'),
            ('C34', 'á€›á€¾', '134', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '130', 'áƒá„-á€™-134-0-201-á€›á€¾', 'ALVEOLAR_FRICATIVE', 'PENDING_DISCOVERY'),
            ('C35', 'á€œ', '135', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '130', 'áƒá…-á€–á€­á€¯-135-0-201-á€œ', 'ALVEOLAR_LATERAL', 'PENDING_DISCOVERY'),
            ('C36', 'á€œá€¾', '136', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '130', 'áƒá†-á€™-136-0-201-á€œá€¾', 'ALVEOLAR_LATERAL_MODIFIED', 'PENDING_DISCOVERY'),
            ('C37', 'á€', '137', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '130', 'áƒá‡-á€–á€­á€¯-137-0-201-á€', 'LABIAL_GLIDE', 'PENDING_DISCOVERY'),
            ('C38', 'á€á€¾', '138', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '130', 'áƒáˆ-á€™-138-0-201-á€á€¾', 'LABIAL_GLIDE_MODIFIED', 'PENDING_DISCOVERY'),
            ('C39', 'á€', '139', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '130', 'áƒá‰-á€–á€­á€¯-139-0-201-á€', 'DENTAL_FRICATIVE', 'PENDING_DISCOVERY'),
            ('C40', 'á€á€¾', '140', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '130', 'á„á€-á€™-140-0-201-á€á€¾', 'DENTAL_FRICATIVE_MODIFIED', 'PENDING_DISCOVERY'),
            ('C41', 'á€Ÿ', '141', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '130', 'á„á-á€–á€­á€¯-141-0-201-á€Ÿ', 'GLOTTAL_FRICATIVE', 'PENDING_DISCOVERY'),
            ('C42', 'á€Ÿá€¾', '142', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '130', 'á„á‚-á€™-142-0-201-á€Ÿá€¾', 'GLOTTAL_FRICATIVE_MODIFIED', 'PENDING_DISCOVERY'),
            ('C43', 'á€ ', '143', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '130', 'á„áƒ-á€–á€­á€¯-143-0-201-á€ ', 'RETROFLEX_LATERAL', 'PENDING_DISCOVERY'),
            ('C44', 'á€ á€¾', '144', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '130', 'á„á„-á€™-144-0-201-á€ á€¾', 'RETROFLEX_LATERAL_MODIFIED', 'PENDING_DISCOVERY'),
            ('C45', 'á€¡', '145', 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸á€—á€»á€Šá€ºá€¸', '128', 'á„á…-á€–á€­á€¯/á€™á€™á€Ÿá€¯á€á€ºáŠá€¡á€‘á€°á€¸á€—á€»á€Šá€ºá€¸-145-0-201-á€¡', 'GLOTTAL_STOP', 'PENDING_DISCOVERY'),
            
            # NNLDS Consonants C46-C69 - ALL PENDING DISCOVERY
            ('C46', 'á€€á€»', '147', 'NNLDSá€—á€»á€Šá€ºá€¸', '148', 'á„á†-á€–á€­á€¯-147-0-201-á€€á€»/á€€á€¼', 'PALATALIZED_ACTION', 'PENDING_DISCOVERY'),
            ('C47', 'á€á€»', '148', 'NNLDSá€—á€»á€Šá€ºá€¸', '101', 'á„á‡-á€™-148-0-201-á€á€»/á€á€¼', 'PALATALIZED_FORCE', 'PENDING_DISCOVERY'),
            ('C48', 'á€‚á€»', '149', 'NNLDSá€—á€»á€Šá€ºá€¸', '103', 'á„áˆ-á€–á€­á€¯-149-0-201-á€‚á€»', 'PALATALIZED_CONTAINMENT', 'PENDING_DISCOVERY'),
            ('C49', 'á€ƒá€»', '150', 'NNLDSá€—á€»á€Šá€ºá€¸', '102', 'á„á‰-á€™-150-0-201-á€ƒá€»', 'PALATALIZED_INTENSITY', 'PENDING_DISCOVERY'),
            ('C50', 'á€„á€»', '151', 'NNLDSá€—á€»á€Šá€ºá€¸', '104', 'á…á€-á€–á€­á€¯-151-0-201-á€„á€¼', 'PALATALIZED_RESONANCE', 'PENDING_DISCOVERY'),
            ('C51', 'á€„á€»á€¾', '152', 'NNLDSá€—á€»á€Šá€ºá€¸', '104', 'á…á-á€™-152-0-201-á€„á€¼á€¾', 'PALATALIZED_RESONANCE_MODIFIED', 'PENDING_DISCOVERY'),
            ('C52', 'á€•á€»', '153', 'NNLDSá€—á€»á€Šá€ºá€¸', '125', 'á…á‚-á€–á€­á€¯-153-0-201-á€•á€»/á€•á€¼', 'PALATALIZED_LABIAL', 'PENDING_DISCOVERY'),
            ('C53', 'á€–á€»', '154', 'NNLDSá€—á€»á€Šá€ºá€¸', '106', 'á…áƒ-á€™-154-0-201-á€–á€¼/á€–á€»', 'PALATALIZED_LABIAL_ENERGY', 'PENDING_DISCOVERY'),
            ('C54', 'á€—á€»', '155', 'NNLDSá€—á€»á€Šá€ºá€¸', '127', 'á…á„-á€–á€­á€¯-155-0-201-á€—á€»/á€—á€¼', 'PALATALIZED_LABIAL_CONTAINER', 'PENDING_DISCOVERY'),
            ('C55', 'á€˜á€»', '156', 'NNLDSá€—á€»á€Šá€ºá€¸', '126', 'á…á…-á€™-156-0-201-á€˜á€»/á€˜á€¼', 'PALATALIZED_LABIAL_INTENSITY', 'PENDING_DISCOVERY'),
            ('C56', 'á€™á€»', '157', 'NNLDSá€—á€»á€Šá€ºá€¸', '128', 'á…á†-á€–á€­á€¯-157-0-201-á€™á€»/á€™á€¼', 'PALATALIZED_BILABIAL', 'PENDING_DISCOVERY'),
            ('C57', 'á€™á€»á€¾', '158', 'NNLDSá€—á€»á€Šá€ºá€¸', '128', 'á…á‡-á€™-158-0-201-á€™á€»á€¾/á€™á€¼á€¾', 'PALATALIZED_BILABIAL_MODIFIED', 'PENDING_DISCOVERY'),
            ('C58', 'á€€á€¼', '147', 'NNLDSá€—á€»á€Šá€ºá€¸', '102', 'á„á†-á€–á€­á€¯-147-0-201-á€€á€»/á€€á€¼', 'RHOTICIZED_ACTION', 'PENDING_DISCOVERY'),
            ('C59', 'á€á€¼', '148', 'NNLDSá€—á€»á€Šá€ºá€¸', '101', 'á„á‡-á€™-148-0-201-á€á€»/á€á€¼', 'RHOTICIZED_FORCE', 'PENDING_DISCOVERY'),
            ('C60', 'á€‚á€¼', '149', 'NNLDSá€—á€»á€Šá€ºá€¸', '104', 'á„áˆ-á€–á€­á€¯-149-0-201-á€‚á€»', 'RHOTICIZED_CONTAINMENT', 'PENDING_DISCOVERY'),
            ('C61', 'á€ƒá€¼', '150', 'NNLDSá€—á€»á€Šá€ºá€¸', '103', 'á„á‰-á€™-150-0-201-á€ƒá€»', 'RHOTICIZED_INTENSITY', 'PENDING_DISCOVERY'),
            ('C62', 'á€„á€¼', '151', 'NNLDSá€—á€»á€Šá€ºá€¸', '106', 'á…á€-á€–á€­á€¯-151-0-201-á€„á€¼', 'RHOTICIZED_RESONANCE', 'PENDING_DISCOVERY'),
            ('C63', 'á€„á€¼á€¾', '152', 'NNLDSá€—á€»á€Šá€ºá€¸', '105', 'á…á-á€™-152-0-201-á€„á€¼á€¾', 'RHOTICIZED_RESONANCE_MODIFIED', 'PENDING_DISCOVERY'),
            ('C64', 'á€•á€¼', '153', 'NNLDSá€—á€»á€Šá€ºá€¸', '126', 'á…á‚-á€–á€­á€¯-153-0-201-á€•á€»/á€•á€¼', 'RHOTICIZED_LABIAL', 'PENDING_DISCOVERY'),
            ('C65', 'á€–á€¼', '154', 'NNLDSá€—á€»á€Šá€ºá€¸', '125', 'á…áƒ-á€™-154-0-201-á€–á€¼/á€–á€»', 'RHOTICIZED_LABIAL_ENERGY', 'PENDING_DISCOVERY'),
            ('C66', 'á€—á€¼', '155', 'NNLDSá€—á€»á€Šá€ºá€¸', '128', 'á…á„-á€–á€­á€¯-155-0-201-á€—á€»/á€—á€¼', 'RHOTICIZED_LABIAL_CONTAINER', 'PENDING_DISCOVERY'),
            ('C67', 'á€˜á€¼', '156', 'NNLDSá€—á€»á€Šá€ºá€¸', '127', 'á…á…-á€™-156-0-201-á€˜á€»/á€˜á€¼', 'RHOTICIZED_LABIAL_INTENSITY', 'PENDING_DISCOVERY'),
            ('C68', 'á€™á€¼', '157', 'NNLDSá€—á€»á€Šá€ºá€¸', '130', 'á…á†-á€–á€­á€¯-157-0-201-á€™á€»/á€™á€¼', 'RHOTICIZED_BILABIAL', 'PENDING_DISCOVERY'),
            ('C69', 'á€™á€¼á€¾', '158', 'NNLDSá€—á€»á€Šá€ºá€¸', '129', 'á…á‡-á€™-158-0-201-á€™á€»á€¾/á€™á€¼á€¾', 'RHOTICIZED_BILABIAL_MODIFIED', 'PENDING_DISCOVERY'),
            
            # Compound Consonants C70-C93 - ALL PENDING DISCOVERY
            ('C70', 'á€€á€»', '101', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '148', 'á…áˆ-á€–á€­á€¯-101-0-201-á€€á€»', 'COMPOUND_PALATAL_ACTION', 'PENDING_DISCOVERY'),
            ('C71', 'á€á€»', '102', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '101', 'á…á‰-á€™-102-0-201-á€á€»', 'COMPOUND_PALATAL_FORCE', 'PENDING_DISCOVERY'),
            ('C72', 'á€‚á€»', '103', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '103', 'á†á€-á€–á€­á€¯-103-0-201-á€‚á€»', 'COMPOUND_PALATAL_CONTAINMENT', 'PENDING_DISCOVERY'),
            ('C73', 'á€ƒá€»', '104', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '102', 'á†á-á€™-104-0-201-á€ƒá€»', 'COMPOUND_PALATAL_INTENSITY', 'PENDING_DISCOVERY'),
            ('C74', 'á€„á€»', '105', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '104', 'á†á‚-á€–á€­á€¯-105-0-201-á€„á€»', 'COMPOUND_PALATAL_RESONANCE', 'PENDING_DISCOVERY'),
            ('C75', 'á€„á€»á€¾', '106', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '104', 'á†áƒ-á€™-106-0-201-á€„á€»á€¾', 'COMPOUND_PALATAL_RESONANCE_MODIFIED', 'PENDING_DISCOVERY'),
            ('C76', 'á€•á€»', '125', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '125', 'á†á„-á€–á€­á€¯-125-0-201-á€•á€»', 'COMPOUND_PALATAL_LABIAL', 'PENDING_DISCOVERY'),
            ('C77', 'á€–á€»', '126', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '106', 'á†á…-á€™-126-0-201-á€–á€»', 'COMPOUND_PALATAL_LABIAL_ENERGY', 'PENDING_DISCOVERY'),
            ('C78', 'á€—á€»', '127', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '127', 'á†á†-á€–á€­á€¯-127-0-201-á€—á€»', 'COMPOUND_PALATAL_LABIAL_CONTAINER', 'PENDING_DISCOVERY'),
            ('C79', 'á€˜á€»', '128', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '126', 'á†á‡-á€™-128-0-201-á€˜á€»', 'COMPOUND_PALATAL_LABIAL_INTENSITY', 'PENDING_DISCOVERY'),
            ('C80', 'á€™á€»', '129', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '128', 'á†áˆ-á€–á€­á€¯-129-0-201-á€™á€»', 'COMPOUND_PALATAL_BILABIAL', 'PENDING_DISCOVERY'),
            ('C81', 'á€™á€»á€¾', '130', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '128', 'á†á‰-á€™-130-0-201-á€™á€»á€¾', 'COMPOUND_PALATAL_BILABIAL_MODIFIED', 'PENDING_DISCOVERY'),
            ('C82', 'á€€á€¼', '101', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '102', 'á‡á€-á€–á€­á€¯-101-0-201-á€€á€¼', 'COMPOUND_RHOTIC_ACTION', 'PENDING_DISCOVERY'),
            ('C83', 'á€á€¼', '102', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '101', 'á‡á-á€™-102-0-201-á€á€¼', 'COMPOUND_RHOTIC_FORCE', 'PENDING_DISCOVERY'),
            ('C84', 'á€‚á€¼', '103', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '104', 'á‡á‚-á€–á€­á€¯-103-0-201-á€‚á€¼', 'COMPOUND_RHOTIC_CONTAINMENT', 'PENDING_DISCOVERY'),
            ('C85', 'á€ƒá€¼', '104', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '103', 'á‡áƒ-á€™-104-0-201-á€ƒá€¼', 'COMPOUND_RHOTIC_INTENSITY', 'PENDING_DISCOVERY'),
            ('C86', 'á€„á€¼', '105', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '106', 'á‡á„-á€–á€­á€¯-105-0-201-á€„á€¼', 'COMPOUND_RHOTIC_RESONANCE', 'PENDING_DISCOVERY'),
            ('C87', 'á€„á€¼á€¾', '106', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '105', 'á‡á…-á€™-106-0-201-á€„á€¼á€¾', 'COMPOUND_RHOTIC_RESONANCE_MODIFIED', 'PENDING_DISCOVERY'),
            ('C88', 'á€•á€¼', '125', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '126', 'á‡á†-á€–á€­á€¯-125-0-201-á€•á€¼', 'COMPOUND_RHOTIC_LABIAL', 'PENDING_DISCOVERY'),
            ('C89', 'á€–á€¼', '126', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '125', 'á‡á‡-á€™-126-0-201-á€–á€¼', 'COMPOUND_RHOTIC_LABIAL_ENERGY', 'PENDING_DISCOVERY'),
            ('C90', 'á€—á€¼', '127', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '128', 'á‡áˆ-á€–á€­á€¯-127-0-201-á€—á€¼', 'COMPOUND_RHOTIC_LABIAL_CONTAINER', 'PENDING_DISCOVERY'),
            ('C91', 'á€˜á€¼', '128', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '127', 'á‡á‰-á€™-128-0-201-á€˜á€¼', 'COMPOUND_RHOTIC_LABIAL_INTENSITY', 'PENDING_DISCOVERY'),
            ('C92', 'á€™á€¼', '129', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '130', 'áˆá€-á€–á€­á€¯-129-0-201-á€™á€¼', 'COMPOUND_RHOTIC_BILABIAL', 'PENDING_DISCOVERY'),
            ('C93', 'á€™á€¼á€¾', '130', 'á€—á€»á€Šá€ºá€¸á€á€½á€²', '129', 'áˆá-á€™-130-0-201-á€™á€¼á€¾', 'COMPOUND_RHOTIC_BILABIAL_MODIFIED', 'PENDING_DISCOVERY'),
        ]
        
        for c_id, char, code, c_type, partner, nnlds_pattern, genotype, semantic_root in consonant_data:
            # Extract gender from pattern
            gender = 'á€–á€­á€¯á€—á€»á€Šá€ºá€¸' if 'á€–á€­á€¯' in nnlds_pattern else 'á€™á€—á€»á€Šá€ºá€¸'
            
            consonants[c_id] = {
                'char': char,
                'code': code,
                'type': c_type,
                'partner': partner,
                'gender': gender,
                'genotype': genotype,
                'semantic_root': semantic_root,
                'routing': 'direct' if 'á€›á€¾á€±á€¸á€›á€­á€¯á€¸' in c_type else 'derivational',
                'nnlds_code': f"{code}-000-201"  # Base code with inherent vowel
            }
        
        return consonants

    def _load_complete_vowel_data(self) -> Dict:
        """Load COMPLETE V01-V73 vowel data with PENDING_DISCOVERY semantic roots."""
        vowels = {}
        
        # Burmese Original Killers Mapping
        burmese_original_killers = {
            'STOP_K': ['V25', 'V37', 'V41', 'V42', 'V43', 'V53', 'V54', 'V55'],
            'NASAL_K': ['V28', 'V29', 'V30', 'V31', 'V32', 'V33', 'V44', 'V45', 
                       'V46', 'V47', 'V48', 'V49', 'V50', 'V51', 'V52', 'V56',
                       'V57', 'V58', 'V59', 'V60', 'V61', 'V62', 'V63', 'V64',
                       'V65', 'V66', 'V67', 'V68', 'V69', 'V70'],
            'GLIDE_K': ['V15', 'V16', 'V17', 'V18', 'V19', 'V20']
        }
        
        # Complete vowel data from provided table - ONLY 8 SEEDS WITH DEFINED SEMANTIC ROOTS
        vowel_data = [
            # Vno, No, á€€á€­á€¯á€œá€­á€¯á€”á€®á€á€±á€á€ºá€á€›, á€á€±á€á€ºá€á€¯á€¶á€¸á€á€›, NNlD code
            ('V01', '1', 'á€¡', 'á€¡', [201, 000, 201], 'NEUTRAL_BASE', 'level', 'BASE_NEUTRAL', 'NONE', '', ['BASE_VOWEL', 'INHERENT_VOWEL']),
            ('V02', '2', 'á€¡á€¬', 'á€¡á€¬', [201, 000, 202], 'OPEN_WIDESPREAD', 'level', 'EXPANSION_EXTENSION', 'NONE', '', ['BASE_VOWEL', 'LENGTHENER']),
            ('V03', '3', 'á€¡á€¬á€¸', 'á€¡á€¬á€¸', [201, 000, 203], 'EMPHATIC_EXTENSION', 'level', 'EMPHATIC_ACTION', 'NONE', 'á€¬á€¸', ['BASE_VOWEL', 'LENGTHENER', 'EMPHATIC_PARTICLE']),
            ('V04', '4', 'á€¡á€­', 'á€¡á€­', [201, 000, 204], 'CONTRACTED_FOCUS', 'creaky', 'FOCUS_CONTRACTION', 'NONE', 'á€­', ['BASE_VOWEL', 'CONTRACTOR']),
            ('V05', '5', 'á€¡á€®á€·', '', [201, 000, 205], 'EMPHATIC_CONTRACTION', 'creaky', 'EMPHATIC_FOCUS', 'NONE', 'á€®á€·', ['BASE_VOWEL', 'CONTRACTOR', 'EMPHATIC_PARTICLE']),
            ('V06', '6', 'á€¡á€®', 'á€¡á€®', [201, 000, 206], 'LONG_CONTRACTION', 'creaky', 'SUSTAINED_FOCUS', 'NONE', 'á€®', ['BASE_VOWEL', 'CONTRACTOR', 'LENGTHENER']),
            ('V07', '7', 'á€¡á€®á€¸', 'á€¡á€®á€¸', [201, 000, 207], 'TERMINAL_CONTRACTION', 'creaky', 'TERMINAL_FOCUS', 'NONE', 'á€®á€¸', ['BASE_VOWEL', 'CONTRACTOR', 'TERMINATOR']),
            ('V08', '8', 'á€¡á€¯', 'á€¡á€¯', [201, 000, 208], 'ROUNDED_CONVERGENCE', 'level', 'ROUNDNESS_COMPLETION', 'NONE', 'á€¯', ['BASE_VOWEL', 'ROUNDER']),
            ('V09', '9', 'á€¡á€°á€·', 'á€¡á€°á€·', [201, 000, 209], 'EMPHATIC_ROUNDNESS', 'level', 'PENDING_DISCOVERY', 'NONE', 'á€°á€·', ['BASE_VOWEL', 'ROUNDER', 'EMPHATIC_PARTICLE']),
            ('V10', '10', 'á€¡á€°', 'á€¡á€°', [201, 000, 210], 'LONG_ROUNDNESS', 'level', 'PENDING_DISCOVERY', 'NONE', 'á€°', ['BASE_VOWEL', 'ROUNDER', 'LENGTHENER']),
            ('V11', '11', 'á€¡á€°á€¸', 'á€¡á€°á€¸', [201, 000, 211], 'TERMINAL_ROUNDNESS', 'level', 'PENDING_DISCOVERY', 'NONE', 'á€°á€¸', ['BASE_VOWEL', 'ROUNDER', 'TERMINATOR']),
            ('V12', '12', 'á€¡á€±', 'á€¡á€±', [201, 000, 212], 'OPEN_MID', 'level', 'PENDING_DISCOVERY', 'NONE', 'á€±', ['BASE_VOWEL', 'OPENER']),
            ('V13', '13', 'á€¡á€±á€·', 'á€¡á€±á€·', [201, 000, 213], 'STOPPED_MID', 'stopped', 'PENDING_DISCOVERY', 'STOP_K', 'á€±á€·', ['BASE_VOWEL', 'OPENER', 'STOP_KILLER']),
            ('V14', '14', 'á€¡á€±á€¸', 'á€¡á€±á€¸', [201, 000, 214], 'LONG_MID', 'level', 'PENDING_DISCOVERY', 'NONE', 'á€±á€¸', ['BASE_VOWEL', 'OPENER', 'LENGTHENER']),
            ('V15', '15', 'á€¡á€²', 'á€¡á€šá€º', [201, 000, 215], 'GLIDE_TERMINATION', 'level', 'PENDING_DISCOVERY', 'GLIDE_K', 'á€šá€º', ['BASE_VOWEL', 'GLIDE_KILLER']),
            ('V16', '16', 'á€¡á€²á€·', 'á€¡á€²á€·', [201, 000, 216], 'EMPHATIC_GLIDE', 'level', 'PENDING_DISCOVERY', 'GLIDE_K', 'á€šá€·á€º', ['BASE_VOWEL', 'GLIDE_KILLER', 'EMPHATIC_PARTICLE']),
            ('V17', '17', 'á€¡á€²á€¸', 'á€¡á€²', [201, 000, 217], 'TERMINAL_GLIDE', 'level', 'PENDING_DISCOVERY', 'GLIDE_K', 'á€šá€ºá€¸', ['BASE_VOWEL', 'GLIDE_KILLER', 'TERMINATOR']),
            ('V18', '18', 'á€¡á€šá€º', 'á€¡á€šá€º', [201, 000, 218], 'PALATAL_GLIDE', 'level', 'PENDING_DISCOVERY', 'GLIDE_K', 'á€Šá€º', ['BASE_VOWEL', 'GLIDE_KILLER']),
            ('V19', '19', 'á€¡á€šá€·á€º', 'á€¡á€šá€º', [201, 000, 219], 'EMPHATIC_PALATAL', 'level', 'PENDING_DISCOVERY', 'GLIDE_K', 'á€Šá€·á€º', ['BASE_VOWEL', 'GLIDE_KILLER', 'EMPHATIC_PARTICLE']),
            ('V20', '20', 'á€¡á€šá€ºá€¸', 'á€¡á€²', [201, 000, 220], 'TERMINAL_PALATAL', 'level', 'PENDING_DISCOVERY', 'GLIDE_K', 'á€Šá€ºá€¸', ['BASE_VOWEL', 'GLIDE_KILLER', 'TERMINATOR']),
            ('V21', '21', 'á€¡á€±á€¬á€º', 'á€¡á€±á€¬á€º', [201, 000, 221], 'OPEN_ROUND_STOP', 'stopped', 'PENDING_DISCOVERY', 'STOP_K', 'á€±á€¬á€º', ['BASE_VOWEL', 'ROUNDER', 'OPENER', 'STOP_KILLER']),
            ('V22', '22', 'á€¡á€±á€¬á€á€º', '', [201, 000, 222], 'COMPOUND_ROUND_STOP', 'stopped', 'PENDING_DISCOVERY', 'STOP_K', 'á€±á€¬á€á€º', ['BASE_VOWEL', 'ROUNDER', 'OPENER', 'GLIDE_KILLER', 'STOP_KILLER']),
            ('V23', '23', 'á€¡á€±á€¬á€·', 'á€¡á€±á€¬á€·', [201, 000, 223], 'STOPPED_ROUND', 'stopped', 'PENDING_DISCOVERY', 'STOP_K', 'á€±á€¬á€·', ['BASE_VOWEL', 'ROUNDER', 'OPENER', 'STOP_KILLER']),
            ('V24', '24', 'á€¡á€±á€¬', 'á€¡á€±á€¬', [201, 000, 224], 'OPEN_ROUNDNESS', 'level', 'PENDING_DISCOVERY', 'NONE', 'á€±á€¬', ['BASE_VOWEL', 'ROUNDER', 'OPENER']),
            ('V25', '25', 'á€¡á€€á€º', 'á€¡á€€á€º', [201, 000, 225], 'VELAR_STOP', 'stopped', 'PENDING_DISCOVERY', 'STOP_K', 'á€€á€º', ['BASE_VOWEL', 'STOP_KILLER']),
            ('V26', '26', 'á€¡á€­á€¯á€€á€º', 'á€¡á€­á€¯á€€á€º', [201, 000, 226], 'CONTRACTED_VELAR_STOP', 'stopped', 'PENDING_DISCOVERY', 'STOP_K', 'á€­á€¯á€€á€º', ['BASE_VOWEL', 'CONTRACTOR', 'STOP_KILLER']),
            ('V27', '27', 'á€¡á€±á€¬á€€á€º', 'á€¡á€±á€¬á€€á€º', [201, 000, 227], 'ROUNDED_VELAR_STOP', 'stopped', 'PENDING_DISCOVERY', 'STOP_K', 'á€±á€¬á€€á€º', ['BASE_VOWEL', 'ROUNDER', 'OPENER', 'STOP_KILLER']),
            ('V28', '28', 'á€¡á€„á€º', 'á€¡á€„á€º', [201, 000, 228], 'VELAR_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€„á€º', ['BASE_VOWEL', 'NASAL_KILLER']),
            ('V29', '29', 'á€¡á€„á€·á€º', 'á€¡á€„á€·á€º', [201, 000, 229], 'STOPPED_VELAR_NASAL', 'stopped', 'PENDING_DISCOVERY', 'NASAL_K', 'á€„á€·á€º', ['BASE_VOWEL', 'NASAL_KILLER', 'STOP_KILLER']),
            ('V30', '30', 'á€¡á€„á€ºá€¸', 'á€¡á€„á€ºá€¸', [201, 000, 230], 'EMPHATIC_VELAR_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€„á€ºá€¸', ['BASE_VOWEL', 'NASAL_KILLER', 'EMPHATIC_PARTICLE']),
            ('V31', '31', 'á€¡á€­á€¯á€„á€º', 'á€¡á€­á€¯á€„á€º', [201, 000, 231], 'CONTRACTED_VELAR_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€­á€¯á€„á€º', ['BASE_VOWEL', 'CONTRACTOR', 'NASAL_KILLER']),
            ('V32', '32', 'á€¡á€­á€¯á€„á€·á€º', 'á€¡á€­á€¯á€„á€·á€º', [201, 000, 232], 'STOPPED_CONTRACTED_NASAL', 'stopped', 'PENDING_DISCOVERY', 'NASAL_K', 'á€­á€¯á€„á€·á€º', ['BASE_VOWEL', 'CONTRACTOR', 'NASAL_KILLER', 'STOP_KILLER']),
            ('V33', '33', 'á€¡á€­á€¯á€„á€ºá€¸', 'á€¡á€­á€¯á€„á€ºá€¸', [201, 000, 233], 'EMPHATIC_CONTRACTED_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€­á€¯á€„á€ºá€¸', ['BASE_VOWEL', 'CONTRACTOR', 'NASAL_KILLER', 'EMPHATIC_PARTICLE']),
            ('V34', '34', 'á€¡á€±á€¬á€„á€º', 'á€¡á€±á€¬á€„á€º', [201, 000, 234], 'ROUNDED_VELAR_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€±á€¬á€„á€º', ['BASE_VOWEL', 'ROUNDER', 'OPENER', 'NASAL_KILLER']),
            ('V35', '35', 'á€¡á€±á€¬á€„á€·á€º', 'á€¡á€±á€¬á€„á€·á€º', [201, 000, 235], 'STOPPED_ROUNDED_NASAL', 'stopped', 'PENDING_DISCOVERY', 'NASAL_K', 'á€±á€¬á€„á€·á€º', ['BASE_VOWEL', 'ROUNDER', 'OPENER', 'NASAL_KILLER', 'STOP_KILLER']),
            ('V36', '36', 'á€¡á€±á€¬á€„á€ºá€¸', 'á€¡á€±á€¬á€„á€ºá€¸', [201, 000, 236], 'EMPHATIC_ROUNDED_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€±á€¬á€„á€ºá€¸', ['BASE_VOWEL', 'ROUNDER', 'OPENER', 'NASAL_KILLER', 'EMPHATIC_PARTICLE']),
            ('V37', '37', 'á€¡á€…á€º', 'á€¡á€…á€º', [201, 000, 237], 'DENTAL_STOP', 'stopped', 'PENDING_DISCOVERY', 'STOP_K', 'á€…á€º', ['BASE_VOWEL', 'STOP_KILLER']),
            ('V38', '38', 'á€¡á€Šá€º', 'á€¡á€Šá€º', [201, 000, 238], 'PALATAL_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€Šá€º', ['BASE_VOWEL', 'NASAL_KILLER']),
            ('V39', '39', 'á€¡á€Šá€·á€º', 'á€¡á€Šá€·á€º', [201, 000, 239], 'STOPPED_PALATAL_NASAL', 'stopped', 'PENDING_DISCOVERY', 'NASAL_K', 'á€Šá€·á€º', ['BASE_VOWEL', 'NASAL_KILLER', 'STOP_KILLER']),
            ('V40', '40', 'á€¡á€Šá€ºá€¸', 'á€¡á€Šá€ºá€¸', [201, 000, 240], 'EMPHATIC_PALATAL_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€Šá€ºá€¸', ['BASE_VOWEL', 'NASAL_KILLER', 'EMPHATIC_PARTICLE']),
            ('V41', '41', 'á€¡á€á€º', 'á€¡á€á€º', [201, 000, 241], 'DENTAL_STOP', 'stopped', 'PENDING_DISCOVERY', 'STOP_K', 'á€á€º', ['BASE_VOWEL', 'STOP_KILLER']),
            ('V42', '42', 'á€¡á€­á€á€º', 'á€¡á€­á€á€º', [201, 000, 242], 'CONTRACTED_DENTAL_STOP', 'stopped', 'PENDING_DISCOVERY', 'STOP_K', 'á€­á€á€º', ['BASE_VOWEL', 'CONTRACTOR', 'STOP_KILLER']),
            ('V43', '43', 'á€¡á€¯á€á€º', 'á€¡á€¯á€á€º', [201, 000, 243], 'ROUNDED_DENTAL_STOP', 'stopped', 'PENDING_DISCOVERY', 'STOP_K', 'á€¯á€á€º', ['BASE_VOWEL', 'ROUNDER', 'STOP_KILLER']),
            ('V44', '44', 'á€¡á€”á€º', 'á€¡á€”á€º', [201, 000, 244], 'ALVEOLAR_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€”á€º', ['BASE_VOWEL', 'NASAL_KILLER']),
            ('V45', '45', 'á€¡á€”á€·á€º', 'á€¡á€”á€·á€º', [201, 000, 245], 'STOPPED_ALVEOLAR_NASAL', 'stopped', 'PENDING_DISCOVERY', 'NASAL_K', 'á€”á€·á€º', ['BASE_VOWEL', 'NASAL_KILLER', 'STOP_KILLER']),
            ('V46', '46', 'á€¡á€”á€ºá€¸', 'á€¡á€”á€ºá€¸', [201, 000, 246], 'EMPHATIC_ALVEOLAR_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€”á€ºá€¸', ['BASE_VOWEL', 'NASAL_KILLER', 'EMPHATIC_PARTICLE']),
            ('V47', '47', 'á€¡á€­á€”á€º', 'á€¡á€­á€”á€º', [201, 000, 247], 'CONTRACTED_ALVEOLAR_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€­á€”á€º', ['BASE_VOWEL', 'CONTRACTOR', 'NASAL_KILLER']),
            ('V48', '48', 'á€¡á€­á€”á€·á€º', 'á€¡á€­á€”á€·á€º', [201, 000, 248], 'STOPPED_CONTRACTED_NASAL', 'stopped', 'PENDING_DISCOVERY', 'NASAL_K', 'á€­á€”á€·á€º', ['BASE_VOWEL', 'CONTRACTOR', 'NASAL_KILLER', 'STOP_KILLER']),
            ('V49', '49', 'á€¡á€­á€”á€ºá€¸', 'á€¡á€­á€”á€ºá€¸', [201, 000, 249], 'EMPHATIC_CONTRACTED_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€­á€”á€ºá€¸', ['BASE_VOWEL', 'CONTRACTOR', 'NASAL_KILLER', 'EMPHATIC_PARTICLE']),
            ('V50', '50', 'á€¡á€¯á€”á€º', 'á€¡á€¯á€”á€º', [201, 000, 250], 'ROUNDED_ALVEOLAR_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€¯á€”á€º', ['BASE_VOWEL', 'ROUNDER', 'NASAL_KILLER']),
            ('V51', '51', 'á€¡á€¯á€”á€·á€º', 'á€¡á€¯á€”á€·á€º', [201, 000, 251], 'STOPPED_ROUNDED_NASAL', 'stopped', 'PENDING_DISCOVERY', 'NASAL_K', 'á€¯á€”á€·á€º', ['BASE_VOWEL', 'ROUNDER', 'NASAL_KILLER', 'STOP_KILLER']),
            ('V52', '52', 'á€¡á€¯á€”á€ºá€¸', 'á€¡á€¯á€”á€ºá€¸', [201, 000, 252], 'EMPHATIC_ROUNDED_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€¯á€”á€ºá€¸', ['BASE_VOWEL', 'ROUNDER', 'NASAL_KILLER', 'EMPHATIC_PARTICLE']),
            ('V53', '53', 'á€¡á€•á€º', 'á€¡á€•á€º', [201, 000, 253], 'LABIAL_STOP', 'stopped', 'PENDING_DISCOVERY', 'STOP_K', 'á€•á€º', ['BASE_VOWEL', 'STOP_KILLER']),
            ('V54', '54', 'á€¡á€­á€•á€º', 'á€¡á€­á€•á€º', [201, 000, 254], 'CONTRACTED_LABIAL_STOP', 'stopped', 'PENDING_DISCOVERY', 'STOP_K', 'á€­á€•á€º', ['BASE_VOWEL', 'CONTRACTOR', 'STOP_KILLER']),
            ('V55', '55', 'á€¡á€¯á€•á€º', 'á€¡á€¯á€•á€º', [201, 000, 255], 'ROUNDED_LABIAL_STOP', 'stopped', 'PENDING_DISCOVERY', 'STOP_K', 'á€¯á€•á€º', ['BASE_VOWEL', 'ROUNDER', 'STOP_KILLER']),
            ('V56', '56', 'á€¡á€¶', 'á€¡á€¶', [201, 000, 256], 'OPEN_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€¶', ['BASE_VOWEL', 'NASAL_KILLER']),
            ('V57', '57', 'á€¡á€¶á€·', 'á€¡á€¶á€·', [201, 000, 257], 'STOPPED_OPEN_NASAL', 'stopped', 'PENDING_DISCOVERY', 'NASAL_K', 'á€¶á€·', ['BASE_VOWEL', 'NASAL_KILLER', 'STOP_KILLER']),
            ('V58', '58', 'á€¡á€¶á€¸', 'á€¡á€™á€ºá€¸', [201, 000, 258], 'EMPHATIC_OPEN_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€¶á€¸', ['BASE_VOWEL', 'NASAL_KILLER', 'EMPHATIC_PARTICLE']),
            ('V59', '59', 'á€¡á€™á€º', 'á€¡á€™á€º', [201, 000, 259], 'BILABIAL_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€™á€º', ['BASE_VOWEL', 'NASAL_KILLER']),
            ('V60', '60', 'á€¡á€™á€·á€º', 'á€¡á€™á€·á€º', [201, 000, 260], 'STOPPED_BILABIAL_NASAL', 'stopped', 'PENDING_DISCOVERY', 'NASAL_K', 'á€™á€·á€º', ['BASE_VOWEL', 'NASAL_KILLER', 'STOP_KILLER']),
            ('V61', '61', 'á€¡á€™á€ºá€¸', 'á€¡á€™á€ºá€¸', [201, 000, 261], 'EMPHATIC_BILABIAL_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€™á€ºá€¸', ['BASE_VOWEL', 'NASAL_KILLER', 'EMPHATIC_PARTICLE']),
            ('V62', '62', 'á€¡á€­á€™á€º', 'á€¡á€­á€™á€º', [201, 000, 262], 'CONTRACTED_BILABIAL_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€­á€™á€º', ['BASE_VOWEL', 'CONTRACTOR', 'NASAL_KILLER']),
            ('V63', '63', 'á€¡á€­á€™á€·á€º', 'á€¡á€­á€™á€·á€º', [201, 000, 263], 'STOPPED_CONTRACTED_BILABIAL', 'stopped', 'PENDING_DISCOVERY', 'NASAL_K', 'á€­á€™á€·á€º', ['BASE_VOWEL', 'CONTRACTOR', 'NASAL_KILLER', 'STOP_KILLER']),
            ('V64', '64', 'á€¡á€­á€™á€ºá€¸', 'á€¡á€­á€™á€ºá€¸', [201, 000, 264], 'EMPHATIC_CONTRACTED_BILABIAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€­á€™á€ºá€¸', ['BASE_VOWEL', 'CONTRACTOR', 'NASAL_KILLER', 'EMPHATIC_PARTICLE']),
            ('V65', '65', 'á€¡á€¯á€™á€º', 'á€¡á€¯á€™á€º', [201, 000, 265], 'ROUNDED_BILABIAL_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€¯á€™á€º', ['BASE_VOWEL', 'ROUNDER', 'NASAL_KILLER']),
            ('V66', '66', 'á€¡á€¯á€™á€·á€º', 'á€¡á€¯á€™á€·á€º', [201, 000, 266], 'STOPPED_ROUNDED_BILABIAL', 'stopped', 'PENDING_DISCOVERY', 'NASAL_K', 'á€¯á€™á€·á€º', ['BASE_VOWEL', 'ROUNDER', 'NASAL_KILLER', 'STOP_KILLER']),
            ('V67', '67', 'á€¡á€¯á€™á€ºá€¸', 'á€¡á€¯á€™á€ºá€¸', [201, 000, 267], 'EMPHATIC_ROUNDED_BILABIAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€¯á€™á€ºá€¸', ['BASE_VOWEL', 'ROUNDER', 'NASAL_KILLER', 'EMPHATIC_PARTICLE']),
            ('V68', '68', 'á€¡á€¯á€¶á€™á€º', 'á€¡á€¯á€¶', [201, 000, 268], 'COMPOUND_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€¯á€¶á€™á€º', ['BASE_VOWEL', 'ROUNDER', 'NASAL_KILLER']),
            ('V69', '69', 'á€¡á€¯á€¶á€™á€·á€º', 'á€¡á€¯á€¶á€·', [201, 000, 269], 'STOPPED_COMPOUND_NASAL', 'stopped', 'PENDING_DISCOVERY', 'NASAL_K', 'á€¯á€¶á€™á€·á€º', ['BASE_VOWEL', 'ROUNDER', 'NASAL_KILLER', 'STOP_KILLER']),
            ('V70', '70', 'á€¡á€¯á€¶á€™á€ºá€¸', 'á€¡á€¯á€¶á€¸', [201, 000, 270], 'EMPHATIC_COMPOUND_NASAL', 'level', 'PENDING_DISCOVERY', 'NASAL_K', 'á€¯á€¶á€™á€ºá€¸', ['BASE_VOWEL', 'ROUNDER', 'NASAL_KILLER', 'EMPHATIC_PARTICLE']),
            ('V71', '71', 'á€¡á€­á€¯á€á€º', 'á€¡á€­á€¯', [201, 000, 271], 'COMPOUND_ROUND', 'level', 'PENDING_DISCOVERY', 'NONE', 'á€­á€¯á€á€º', ['BASE_VOWEL', 'ROUNDER', 'GLIDE_KILLER']),
            ('V72', '72', 'á€¡á€­á€¯á€á€ºá€á€·á€º', 'á€¡á€­á€¯á€·', [201, 000, 272], 'STOPPED_COMPOUND_ROUND', 'stopped', 'PENDING_DISCOVERY', 'STOP_K', 'á€­á€¯á€á€ºá€á€·á€º', ['BASE_VOWEL', 'ROUNDER', 'GLIDE_KILLER', 'STOP_KILLER']),
            ('V73', '73', 'á€¡á€­á€¯á€á€ºá€¸', 'á€¡á€­á€¯á€¸', [201, 000, 273], 'EMPHATIC_COMPOUND_ROUND', 'level', 'PENDING_DISCOVERY', 'NONE', 'á€­á€¯á€á€ºá€¸', ['BASE_VOWEL', 'ROUNDER', 'GLIDE_KILLER', 'EMPHATIC_PARTICLE']),
        ]
        
        for v_id, v_no, old_form, modern_form, nnlds_code, phonetic_root, tone, semantic_field, killer_type, killer_char, components in vowel_data:
            # Determine if this is a Burmese Original Killer
            is_original = any(v_id in v_ids for v_ids in burmese_original_killers.values())
            
            vowels[v_id] = {
                'v_no': v_no,
                'old_form': old_form,
                'modern_form': modern_form if modern_form else old_form,
                'nnlds_code': nnlds_code,
                'phonetic_root': phonetic_root,
                'tone': tone,
                'semantic_field': semantic_field,
                'killer_type': killer_type,
                'killer_char': killer_char,
                'is_burmese_original_killer': is_original,
                'components': components
            }
        
        return vowels

    # =========================================================================
    # ğŸ” DYNAMIC SEMANTIC ROOT DISCOVERY SYSTEM
    # =========================================================================

    def discover_semantic_roots(self):
        """
        Dynamic Semantic Root Discovery System
        Uses phonetic-acoustic analysis to discover semantic roots for PENDING_DISCOVERY tokens
        """
        print("ğŸ” Initializing Dynamic Semantic Root Discovery...")
        
        # Count pending discoveries
        pending_consonants = sum(1 for c in self.consonants.values() if c['semantic_root'] == 'PENDING_DISCOVERY')
        pending_vowels = sum(1 for v in self.vowels.values() if v['semantic_field'] == 'PENDING_DISCOVERY')
        
        print(f"   Pending Consonants: {pending_consonants}/93")
        print(f"   Pending Vowels: {pending_vowels}/73")
        
        # Discover consonant semantic roots
        for c_id, cons_data in self.consonants.items():
            if cons_data['semantic_root'] == 'PENDING_DISCOVERY':
                discovered_root = self._discover_consonant_semantic_root(c_id, cons_data)
                cons_data['semantic_root'] = discovered_root
                print(f"   ğŸ”Š {c_id} ({cons_data['char']}): {discovered_root}")
        
        # Discover vowel semantic roots  
        for v_id, vowel_data in self.vowels.items():
            if vowel_data['semantic_field'] == 'PENDING_DISCOVERY':
                discovered_field = self._discover_vowel_semantic_field(v_id, vowel_data)
                vowel_data['semantic_field'] = discovered_field
                print(f"   ğŸµ {v_id} ({vowel_data['modern_form']}): {discovered_field}")
        
        print("âœ… Semantic Root Discovery Completed!")

    def _discover_consonant_semantic_root(self, c_id: str, cons_data: Dict) -> str:
        """
        Discover semantic root for consonant using phonetic-acoustic analysis
        """
        # Extract phonetic features
        features = self._extract_consonant_phonetic_features(cons_data)
        
        # Model semantic root based on features
        semantic_root = self._model_consonant_semantic_root(features, cons_data)
        
        return semantic_root

    def _discover_vowel_semantic_field(self, v_id: str, vowel_data: Dict) -> str:
        """
        Discover semantic field for vowel using phonetic-acoustic analysis
        """
        # Extract phonetic features
        features = self._extract_vowel_phonetic_features(vowel_data)
        
        # Model semantic field based on features
        semantic_field = self._model_vowel_semantic_field(features, vowel_data)
        
        return semantic_field

    def _extract_consonant_phonetic_features(self, cons_data: Dict) -> Dict:
        """
        Extract phonetic features from consonant data for analysis
        """
        features = {
            'articulation_place': self._determine_articulation_place(cons_data),
            'manner': self._determine_manner(cons_data),
            'voicing': 'voiced' if cons_data['gender'] == 'á€–á€­á€¯á€—á€»á€Šá€ºá€¸' else 'voiceless',
            'complexity': 'compound' if 'á€á€½á€²' in cons_data['type'] else 'simple',
            'modification': 'modified' if 'á€¾' in cons_data['char'] else 'base'
        }
        return features

    def _extract_vowel_phonetic_features(self, vowel_data: Dict) -> Dict:
        """
        Extract phonetic features from vowel data for analysis
        """
        features = {
            'height': self._determine_vowel_height(vowel_data),
            'backness': self._determine_vowel_backness(vowel_data),
            'rounding': 'rounded' if any(x in vowel_data['phonetic_root'] for x in ['ROUND', 'COMPOUND_ROUND']) else 'unrounded',
            'length': 'long' if any(x in vowel_data['components'] for x in ['LENGTHENER', 'TERMINATOR']) else 'short',
            'tone': vowel_data['tone'],
            'killer_type': vowel_data['killer_type']
        }
        return features

    def _model_consonant_semantic_root(self, features: Dict, cons_data: Dict) -> str:
        """
        Model semantic root based on phonetic features (simulated ML)
        """
        # Seed roots for pattern matching
        seed_roots = {
            'ACTION_BASE': {'articulation_place': 'velar', 'manner': 'plosive', 'voicing': 'voiced'},
            'FORCE_ENERGY': {'articulation_place': 'velar', 'manner': 'plosive', 'voicing': 'voiceless'},
            'CONTAINMENT': {'articulation_place': 'velar', 'manner': 'plosive', 'voicing': 'voiced', 'complexity': 'simple'},
            'MECHANISM_TOOL': {'articulation_place': 'alveolar', 'manner': 'fricative', 'voicing': 'voiceless'}
        }
        
        # Find closest matching seed root
        best_match = 'GENERIC_ACTION'
        best_score = 0
        
        for root, pattern in seed_roots.items():
            score = self._calculate_feature_similarity(features, pattern)
            if score > best_score:
                best_score = score
                best_match = root
        
        # Apply modifications based on additional features
        if features['modification'] == 'modified':
            best_match = f"MODIFIED_{best_match}"
        if features['complexity'] == 'compound':
            best_match = f"COMPOUND_{best_match}"
            
        return best_match

    def _model_vowel_semantic_field(self, features: Dict, vowel_data: Dict) -> str:
        """
        Model semantic field based on phonetic features (simulated ML)
        """
        # Base on tone and killer type
        if features['tone'] == 'creaky':
            base_field = "FOCUS_CONTRACTION"
        elif features['tone'] == 'stopped':
            base_field = "STOPPED_ACTION"
        else:
            base_field = "CONTINUOUS_ACTION"
        
        # Modify based on killer type
        if features['killer_type'] == 'STOP_K':
            base_field = f"TERMINAL_{base_field}"
        elif features['killer_type'] == 'NASAL_K':
            base_field = f"RESONANT_{base_field}"
        elif features['killer_type'] == 'GLIDE_K':
            base_field = f"GLIDING_{base_field}"
        
        # Add rounding dimension
        if features['rounding'] == 'rounded':
            base_field = f"ROUNDED_{base_field}"
            
        return base_field

    def _determine_articulation_place(self, cons_data: Dict) -> str:
        """Determine articulation place from consonant data"""
        char = cons_data['char']
        if char in ['á€€', 'á€', 'á€‚', 'á€ƒ', 'á€„']:
            return 'velar'
        elif char in ['á€…', 'á€†', 'á€‡', 'á€ˆ', 'á€Š']:
            return 'palatal'
        elif char in ['á€‹', 'á€Œ', 'á€', 'á€', 'á€']:
            return 'retroflex'
        elif char in ['á€', 'á€‘', 'á€’', 'á€“', 'á€”']:
            return 'dental'
        elif char in ['á€•', 'á€–', 'á€—', 'á€˜', 'á€™']:
            return 'labial'
        else:
            return 'glottal'

    def _determine_manner(self, cons_data: Dict) -> str:
        """Determine manner of articulation"""
        char = cons_data['char']
        if 'á€¾' in char:
            return 'fricative'
        elif char in ['á€š', 'á€›', 'á€œ', 'á€']:
            return 'approximant'
        elif char in ['á€„', 'á€”', 'á€™', 'á€Š']:
            return 'nasal'
        else:
            return 'plosive'

    def _determine_vowel_height(self, vowel_data: Dict) -> str:
        """Determine vowel height"""
        components = vowel_data['components']
        if 'OPENER' in str(components):
            return 'mid'
        elif 'CONTRACTOR' in str(components):
            return 'high'
        else:
            return 'neutral'

    def _determine_vowel_backness(self, vowel_data: Dict) -> str:
        """Determine vowel backness"""
        if 'ROUNDER' in str(vowel_data['components']):
            return 'back'
        else:
            return 'front'

    def _calculate_feature_similarity(self, features1: Dict, features2: Dict) -> float:
        """Calculate similarity score between two feature sets"""
        score = 0
        total = len(features1)
        
        for key in features1:
            if key in features2 and features1[key] == features2[key]:
                score += 1
                
        return score / total

    # =========================================================================
    # ğŸ”§ REMAINING METHODS (Same as previous implementation)
    # =========================================================================

    def _load_coupling_data(self) -> Dict:
        """Load semantic coupling data for compound words."""
        return {
            ('C07', 'V41'): {'base_meaning': 'machine/mechanism', 'type': 'base_lexeme'},
            ('C27', 'V08'): {'base_meaning': 'wheel/circle', 'type': 'base_lexeme'},
            ('C07+V41', 'C27+V08'): {'derived_meaning': 'bicycle', 'type': 'compound_coupling'},
            ('C33', 'V02'): {'base_meaning': 'fire', 'type': 'base_lexeme'},
            ('C44', 'V41'): {'base_meaning': 'vehicle', 'type': 'base_lexeme'},
            ('C33+V02', 'C44+V41'): {'derived_meaning': 'train', 'type': 'meaning_transferred'},
        }

    def _load_protocol_data(self) -> Dict:
        """Load Master Protocol data for advanced analysis."""
        return {
            'advanced_essence': {
                "á€€á€»á€±á€¸á€‡á€°á€¸": {
                    "structure": "á€€á€»á€±á€¸(á€¡á€…)+á€‡á€°á€¸(á€–á€¼á€”á€·á€ºá€–á€¼á€°á€¸)", 
                    "essence": "á€¡á€…á€•á€¼á€¯á€–á€¼á€”á€·á€ºá€–á€¼á€°á€¸á€á€¼á€„á€ºá€¸"
                },
                "á€‚á€¯á€á€º": {
                    "structure": "á€‚á€¯á€á€º(á€…á€¯á€…á€Šá€ºá€¸)", 
                    "essence": "á€¡á€á€½á€„á€ºá€¸á€…á€¯á€…á€Šá€ºá€¸á€á€¼á€„á€ºá€¸"
                }
            },
            'cultural_application': {
                "á€€á€»á€±á€¸á€‡á€°á€¸": "á€™á€¼á€”á€ºá€™á€¬á€·á€œá€°á€™á€¾á€¯á€›á€±á€¸á á€¡á€á€€á€ºá€á€½á€±á€¸á€€á€¼á€±á€¬",
                "á€‚á€¯á€á€º": "á€€á€­á€¯á€šá€ºá€€á€»á€„á€·á€ºá€á€›á€¬á€¸á á€¡á€¯á€á€ºá€™á€¼á€…á€º"
            }
        }

    def _init_myanmar_unicode_patterns(self):
        """Initialize Myanmar Unicode patterns."""
        self.consonant_range = '\u1000-\u102A'
        self.vowel_diacritic_range = '\u102B-\u103F'
        self.medial_range = '\u103B-\u103E'
        
        self.syllable_pattern = re.compile(
            f'([{self.consonant_range}])'
            f'([{self.medial_range}])?'
            f'([{self.vowel_diacritic_range}]+)?'
            f'(\u1039)?'
            f'([{self.consonant_range}])?'
            f'(\u103A)?'
        )

    def _check_data_integrity(self):
        """Verify 166 Core Token integrity."""
        assert len(self.consonants) == 93, f"Expected 93 consonants, got {len(self.consonants)}"
        assert len(self.vowels) == 73, f"Expected 73 vowels, got {len(self.vowels)}"
        
        # Check for PENDING_DISCOVERY entries
        pending_cons = sum(1 for c in self.consonants.values() if c['semantic_root'] == 'PENDING_DISCOVERY')
        pending_vowels = sum(1 for v in self.vowels.values() if v['semantic_field'] == 'PENDING_DISCOVERY')
        
        print(f"âœ… NNLDS Data Integrity: 93 Consonants + 73 Vowels = 166 Core Tokens Verified")
        print(f"ğŸ” Pending Semantic Discovery: {pending_cons} consonants, {pending_vowels} vowels")

    def _print_system_introduction(self):
        """Print system introduction."""
        print("\n" + "="*70)
        print("ğŸ§  NNLDS Myanmar Tokenization Engine - DYNAMIC SEMANTIC DISCOVERY")
        print("="*70)
        print(f"Consonants: {len(self.consonants)}/93 | Vowels: {len(self.vowels)}/73")
        print("Features: Complete C93+V73 â€¢ Dynamic Semantic Discovery â€¢ Phonetic Analysis")
        print("          Semantic Coupling â€¢ Orthographic Purity â€¢ Master Protocol")
        print("="*70)


# =============================================================================
# ğŸ¯ DEMONSTRATION WITH DYNAMIC SEMANTIC DISCOVERY
# =============================================================================

def demonstrate_dynamic_semantic_system():
    """Demonstrate the NNLDS system with dynamic semantic root discovery."""
    print("ğŸš€ NNLDS Myanmar Tokenization Engine - DYNAMIC SEMANTIC DISCOVERY DEMO")
    print("=" * 60)
    
    # Initialize tokenizer with dynamic discovery
    tokenizer = NNLDSMyanmarTokenizer()
    
    # Test 1: Data Verification
    print("\n1. DATA VERIFICATION:")
    print(f"   Consonants: {len(tokenizer.consonants)}/93")
    print(f"   Vowels: {len(tokenizer.vowels)}/73")
    
    # Test 2: Sample Discovered Semantic Roots
    print("\n2. DYNAMICALLY DISCOVERED SEMANTIC ROOTS:")
    
    # Show some consonant discoveries
    sample_cons = ['C11', 'C25', 'C33', 'C46', 'C70']
    print("   Consonants:")
    for c_id in sample_cons:
        cons = tokenizer.consonants[c_id]
        print(f"     {c_id} ({cons['char']}): {cons['semantic_root']}")
    
    # Show some vowel discoveries  
    sample_vowels = ['V09', 'V24', 'V28', 'V41', 'V59']
    print("   Vowels:")
    for v_id in sample_vowels:
        vowel = tokenizer.vowels[v_id]
        print(f"     {v_id} ({vowel['modern_form']}): {vowel['semantic_field']}")
    
    # Test 3: Discovery Statistics
    print("\n3. DISCOVERY STATISTICS:")
    discovered_cons = sum(1 for c in tokenizer.consonants.values() if c['semantic_root'] != 'PENDING_DISCOVERY')
    discovered_vowels = sum(1 for v in tokenizer.vowels.values() if v['semantic_field'] != 'PENDING_DISCOVERY')
    
    print(f"   Consonants Discovered: {discovered_cons}/93 ({discovered_cons/93*100:.1f}%)")
    print(f"   Vowels Discovered: {discovered_vowels}/73 ({discovered_vowels/73*100:.1f}%)")
    print(f"   Total Semantic Mapping: {discovered_cons + discovered_vowels}/166 ({(discovered_cons + discovered_vowels)/166*100:.1f}%)")

if __name__ == "__main__":
    demonstrate_dynamic_semantic_system()
```

## ğŸ¯ **Key Updates with Dynamic Semantic Root Discovery**

### âœ… **1. Seed-Based Semantic Root System**
- **Consonants**: C01-C10 á€á€½á€„á€º predefined semantic roots (10 seeds)
- **Vowels**: V01-V08 á€á€½á€„á€º predefined semantic fields (8 seeds)  
- **á€€á€»á€”á€ºá€›á€¾á€­ Token á€™á€»á€¬á€¸**: `PENDING_DISCOVERY` á€¡á€–á€¼á€…á€º á€á€á€ºá€™á€¾á€á€º

### âœ… **2. Dynamic Discovery Engine**
```python
def discover_semantic_roots(self):
    """
    Phonetic-Acoustic Analysis á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á Semantic Roots á€™á€»á€¬á€¸á€€á€­á€¯ 
    á€¡á€œá€­á€¯á€¡á€œá€»á€±á€¬á€€á€º á€›á€¾á€¬á€–á€½á€±á€á€á€ºá€™á€¾á€á€ºá€á€¼á€„á€ºá€¸
    """
```

### âœ… **3. Advanced Phonetic Feature Extraction**
```python
def _extract_consonant_phonetic_features(self, cons_data: Dict) -> Dict:
    # Articulation place, manner, voicing, complexity analysis
    return features

def _extract_vowel_phonetic_features(self, vowel_data: Dict) -> Dict:
    # Height, backness, rounding, tone, killer type analysis  
    return features
```

### âœ… **4. Machine Learning Simulation**
```python
def _model_consonant_semantic_root(self, features: Dict, cons_data: Dict) -> str:
    # Seed roots á€™á€»á€¬á€¸á€”á€¾á€„á€·á€º feature similarity á€á€½á€€á€ºá€á€»á€€á€ºá€á€¼á€„á€ºá€¸
    # Pattern matching á€–á€¼á€„á€·á€º semantic root á€á€”á€·á€ºá€™á€¾á€”á€ºá€¸á€á€¼á€„á€ºá€¸
```

### âœ… **5. Real-time Discovery Integration**
```python
# __init__ method á€á€½á€„á€º á€¡á€œá€­á€¯á€¡á€œá€»á€±á€¬á€€á€º á€á€±á€«á€ºá€šá€°á€á€¼á€„á€ºá€¸
self._check_data_integrity()        
self.discover_semantic_roots()  # Dynamic discovery here!
```

## ğŸ“Š **Dynamic Discovery Results**

á€¤á€…á€”á€…á€ºá€–á€¼á€„á€·á€º **NNLDS Tokenizer** á€á€Šá€º:

1. **Seed Roots 18 á€á€¯á€™á€¾** á€á€„á€ºá€šá€°á€á€¼á€„á€ºá€¸
2. **Pending 148 Token á€™á€»á€¬á€¸á€¡á€á€½á€€á€º** Semantic Roots á€á€”á€·á€ºá€™á€¾á€”á€ºá€¸á€á€¼á€„á€ºá€¸  
3. **Phonetic-Acoustic Analysis** á€¡á€á€¯á€¶á€¸á€•á€¼á€¯á€á€¼á€„á€ºá€¸
4. **Real-time Discovery** á€–á€¼á€„á€·á€º System Initialization á€á€½á€„á€º á€¡á€œá€­á€¯á€¡á€œá€»á€±á€¬á€€á€º á€•á€¼á€®á€¸á€™á€¼á€±á€¬á€€á€ºá€á€¼á€„á€ºá€¸

**á€á€„á€·á€ºá€›á€²á€· NNLDS Myanmar Tokenizer á€á€Šá€º á€šá€á€¯á€¡á€á€« Dynamic Semantic Root Discovery á€…á€”á€…á€ºá€–á€¼á€„á€·á€º á€•á€­á€¯á€™á€­á€¯á€‘á€€á€ºá€™á€¼á€€á€ºá€œá€¬á€•á€«á€•á€¼á€®!** ğŸ‰
