# Myanmar Language Tokenization System (Sar Pone Gyi + NNLDS)

á€€á€»á€±á€¸á€‡á€°á€¸á€•á€«! á€™á€¼á€”á€ºá€™á€¬á€…á€¬ tokenization á€…á€”á€…á€ºá€¡á€á€½á€€á€º GitHub-ready codebase á€€á€­á€¯ á€•á€¼á€”á€ºá€œá€Šá€ºá€á€Šá€ºá€†á€±á€¬á€€á€ºá€•á€±á€¸á€•á€«á€™á€šá€ºá‹

## Project Structure

```
myanmar-tokenizer/
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ tokenizer.py
â”‚   â”œâ”€â”€ nnlds_processor.py
â”‚   â”œâ”€â”€ attention_engine.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ nnlds_codebook.csv
â”‚   â””â”€â”€ sar_pone_gyi_dictionary.json
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ basic_usage.py
â”‚   â””â”€â”€ advanced_processing.py
â””â”€â”€ tests/
    â”œâ”€â”€ test_tokenizer.py
    â””â”€â”€ test_nnlds.py
```

## Core Implementation

### 1. `src/tokenizer.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Myanmar Language Tokenizer based on Sar Pone Gyi method and NNLDS system
Author: AI Assistant with Linguistic Expert
License: MIT
"""

import re
import pandas as pd
import json
from typing import List, Dict, Tuple, Optional

class MyanmarTokenizer:
    """Main tokenizer class combining Sar Pone Gyi and NNLDS approaches"""
    
    def __init__(self, nnlds_csv_path: str, dictionary_path: str):
        self.nnlds_processor = NNLDSProcessor(nnlds_csv_path)
        self.sar_pone_gyi_dict = self._load_sar_pone_gyi_dictionary(dictionary_path)
        self.attention_engine = AttentionEngine()
        
    def _load_sar_pone_gyi_dictionary(self, path: str) -> Dict:
        """Load Sar Pone Gyi token dictionary"""
        with open(path, 'r', encoding='utf-8') as f:
            return json.load(f)
    
    def tokenize(self, text: str, return_metadata: bool = False) -> List:
        """
        Main tokenization function
        
        Args:
            text: Myanmar text to tokenize
            return_metadata: Whether to return detailed metadata
            
        Returns:
            List of tokens with optional metadata
        """
        # Step 1: Basic segmentation
        words = self._segment_text(text)
        
        # Step 2: Tokenize each word
        tokens = []
        for word in words:
            word_tokens = self._tokenize_word(word, return_metadata)
            tokens.extend(word_tokens)
            
        return tokens
    
    def _segment_text(self, text: str) -> List[str]:
        """Segment text into words using Myanmar-specific rules"""
        # Myanmar word boundaries (space, punctuation, etc.)
        segments = re.findall(r'[\u1000-\u109F]+|[^\u1000-\u109F]+', text)
        return [seg for seg in segments if seg.strip()]
    
    def _tokenize_word(self, word: str, return_metadata: bool) -> List:
        """Tokenize a single Myanmar word"""
        # Check for atomic tokens first (compound nouns, special patterns)
        atomic_token = self._check_atomic_tokens(word)
        if atomic_token:
            return [atomic_token] if not return_metadata else [{
                'token': word,
                'type': atomic_token['type'],
                'group': atomic_token['group'],
                'nnlds_triplet': self.nnlds_processor.analyze_word(word),
                'attention_weight': atomic_token['attention_weight']
            }]
        
        # Apply Sar Pone Gyi tokenization rules
        return self._apply_sar_pone_gyi_rules(word, return_metadata)
    
    def _check_atomic_tokens(self, word: str) -> Optional[Dict]:
        """Check if word is an atomic token (compound noun, special pattern)"""
        # Compound nouns check
        if word in self.sar_pone_gyi_dict.get('compound_nouns', {}):
            return {
                'type': 'compound_noun',
                'group': 'group_2',
                'attention_weight': 1.0
            }
        
        # Special patterns check
        if word in self.sar_pone_gyi_dict.get('special_patterns', {}):
            return {
                'type': 'special_pattern', 
                'group': 'group_6',
                'attention_weight': 0.95
            }
            
        return None
    
    def _apply_sar_pone_gyi_rules(self, word: str, return_metadata: bool) -> List:
        """Apply Sar Pone Gyi linguistic rules for tokenization"""
        tokens = []
        i = 0
        
        while i < len(word):
            token_info = self._find_longest_match(word[i:])
            if token_info:
                if return_metadata:
                    token_info['nnlds_triplet'] = self.nnlds_processor.analyze_word(token_info['token'])
                    token_info['attention_config'] = self.attention_engine.get_attention_config(token_info)
                
                tokens.append(token_info if return_metadata else token_info['token'])
                i += len(token_info['token'])
            else:
                # Fallback: character-level tokenization
                token = word[i]
                if return_metadata:
                    tokens.append({
                        'token': token,
                        'type': 'character',
                        'group': 'unknown',
                        'attention_weight': 0.6
                    })
                else:
                    tokens.append(token)
                i += 1
                
        return tokens
    
    def _find_longest_match(self, text: str) -> Optional[Dict]:
        """Find longest matching token from dictionary"""
        # Implementation of longest matching algorithm
        for length in range(min(10, len(text)), 0, -1):
            candidate = text[:length]
            
            # Check in root nouns
            if candidate in self.sar_pone_gyi_dict.get('root_nouns', {}):
                return {
                    'token': candidate,
                    'type': 'root_noun',
                    'group': 'group_1',
                    'attention_weight': 0.9
                }
                
            # Check in root verbs
            if candidate in self.sar_pone_gyi_dict.get('root_verbs', {}):
                return {
                    'token': candidate,
                    'type': 'root_verb', 
                    'group': 'group_3',
                    'attention_weight': 0.9
                }
                
            # Check in particles
            if candidate in self.sar_pone_gyi_dict.get('particles', {}):
                return {
                    'token': candidate,
                    'type': 'particle',
                    'group': 'group_5', 
                    'attention_weight': 0.9
                }
                
        return None

    def analyze_sentence(self, sentence: str) -> Dict:
        """Complete sentence analysis with linguistic metadata"""
        tokens = self.tokenize(sentence, return_metadata=True)
        
        analysis = {
            'original_sentence': sentence,
            'tokens': tokens,
            'linguistic_analysis': self._analyze_linguistic_structure(tokens),
            'attention_map': self.attention_engine.generate_attention_map(tokens)
        }
        
        return analysis
    
    def _analyze_linguistic_structure(self, tokens: List) -> Dict:
        """Analyze linguistic structure of tokenized sentence"""
        structure = {
            'subject': None,
            'verb': None,
            'object': None,
            'particles': [],
            'semantic_units': []
        }
        
        for token in tokens:
            if token.get('type') == 'root_noun':
                if not structure['subject']:
                    structure['subject'] = token
                else:
                    structure['object'] = token
            elif token.get('type') == 'root_verb':
                structure['verb'] = token
            elif token.get('type') == 'particle':
                structure['particles'].append(token)
                
            if token.get('nnlds_triplet'):
                structure['semantic_units'].append({
                    'token': token['token'],
                    'semantic_role': token['nnlds_triplet'].get('semantic_role'),
                    'temporal_aspect': token['nnlds_triplet'].get('temporal_aspect')
                })
                
        return structure
```

### 2. `src/nnlds_processor.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NNLDS Code System Processor for Myanmar Language
"""

import pandas as pd
from typing import Dict, List, Optional, Tuple

class NNLDSProcessor:
    """Process NNLDS codes for phonetic and semantic analysis"""
    
    def __init__(self, csv_path: str):
        self.codebook = self._load_codebook(csv_path)
        self.consonants = self._build_consonant_map()
        self.vowels = self._build_vowel_map()
        self.clusters = self._build_cluster_map()
    
    def _load_codebook(self, path: str) -> pd.DataFrame:
        """Load NNLDS codebook CSV"""
        return pd.read_csv(path)
    
    def _build_consonant_map(self) -> Dict:
        """Build consonant code mapping"""
        consonants = self.codebook[self.codebook['Category'] == 'Consonant']
        return {
            row['Symbol']: {
                'code': row['Code'],
                'ipa': row['IPA'],
                'type': row['Type/Tag'],
                'semantic_role': row['Meaning/Role']
            }
            for _, row in consonants.iterrows()
        }
    
    def _build_vowel_map(self) -> Dict:
        """Build vowel code mapping with M4/M3 semantics"""
        vowels = self.codebook[self.codebook['Category'] == 'Vowel']
        return {
            row['Symbol']: {
                'code': row['Code'],
                'ipa': row['IPA'],
                'm4_species': row['Species (M4)'],
                'm3_tone': row['Tone (M3)'],
                'semantic_role': row['Meaning/Role']
            }
            for _, row in vowels.iterrows()
        }
    
    def _build_cluster_map(self) -> Dict:
        """Build cluster code mapping"""
        clusters = self.codebook[self.codebook['Category'] == 'Cluster']
        return {
            row['Symbol']: {
                'code': row['Code'],
                'type': row['Type/Tag'],
                'role': row['Meaning/Role']
            }
            for _, row in clusters.iterrows()
        }
    
    def analyze_word(self, word: str) -> Optional[Dict]:
        """Analyze word using NNLDS system"""
        if not word or not any('\u1000' <= char <= '\u109F' for char in word):
            return None
            
        analysis = {
            'phonetic_breakdown': self._phonetic_analysis(word),
            'semantic_components': self._semantic_analysis(word),
            'temporal_aspect': self._temporal_analysis(word)
        }
        
        return analysis
    
    def _phonetic_analysis(self, word: str) -> List[Dict]:
        """Break down word into phonetic components"""
        components = []
        i = 0
        
        while i < len(word):
            # Look for consonant
            consonant = self._find_consonant(word[i:])
            if consonant:
                components.append({
                    'type': 'consonant',
                    'symbol': consonant,
                    'data': self.consonants.get(consonant, {})
                })
                i += len(consonant)
                continue
                
            # Look for vowel
            vowel = self._find_vowel(word[i:])
            if vowel:
                components.append({
                    'type': 'vowel', 
                    'symbol': vowel,
                    'data': self.vowels.get(vowel, {})
                })
                i += len(vowel)
                continue
                
            # Look for cluster
            cluster = self._find_cluster(word[i:])
            if cluster:
                components.append({
                    'type': 'cluster',
                    'symbol': cluster, 
                    'data': self.clusters.get(cluster, {})
                })
                i += len(cluster)
                continue
                
            # Unknown character
            components.append({
                'type': 'unknown',
                'symbol': word[i],
                'data': {}
            })
            i += 1
            
        return components
    
    def _semantic_analysis(self, word: str) -> Dict:
        """Extract semantic information from word components"""
        components = self._phonetic_analysis(word)
        
        semantic_info = {
            'primary_meaning': None,
            'semantic_category': None,
            'action_type': None,
            'temporal_quality': None
        }
        
        for comp in components:
            if comp['type'] == 'consonant':
                role = comp['data'].get('semantic_role', '')
                if 'Root' in role or 'Base' in role:
                    semantic_info['primary_meaning'] = role
                if 'Action' in role or 'Verb' in role:
                    semantic_info['action_type'] = role
                    
            elif comp['type'] == 'vowel':
                m4_species = comp['data'].get('m4_species', '')
                semantic_info['semantic_category'] = m4_species
                semantic_info['temporal_quality'] = comp['data'].get('m3_tone')
                
        return semantic_info
    
    def _temporal_analysis(self, word: str) -> Dict:
        """Analyze temporal and aspectual information"""
        components = self._phonetic_analysis(word)
        
        temporal_info = {
            'tense_indication': None,
            'aspect_type': None,
            'modality': None
        }
        
        for comp in components:
            if comp['type'] == 'vowel':
                tone = comp['data'].get('m3_tone', '')
                if tone == 'á€·':  # Short impulse
                    temporal_info['aspect_type'] = 'immediate'
                elif tone == 'á€¸':  # Sustained
                    temporal_info['aspect_type'] = 'continuous'
                elif tone == 'á€¹':  # Checked stop
                    temporal_info['aspect_type'] = 'completed'
                    
        return temporal_info
    
    def _find_consonant(self, text: str) -> Optional[str]:
        """Find consonant at beginning of text"""
        for length in range(3, 0, -1):
            candidate = text[:length]
            if candidate in self.consonants:
                return candidate
        return None
    
    def _find_vowel(self, text: str) -> Optional[str]:
        """Find vowel at beginning of text"""
        for length in range(3, 0, -1):
            candidate = text[:length]
            if candidate in self.vowels:
                return candidate
        return None
    
    def _find_cluster(self, text: str) -> Optional[str]:
        """Find cluster at beginning of text"""
        for length in range(2, 0, -1):
            candidate = text[:length]
            if candidate in self.clusters:
                return candidate
        return None
```

### 3. `src/attention_engine.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Attention Mechanism for Myanmar Language Processing
"""

from typing import List, Dict

class AttentionEngine:
    """Attention weight calculation based on linguistic rules"""
    
    def __init__(self):
        self.base_weights = {
            'root_noun': 0.9,
            'compound_noun': 1.0,
            'root_verb': 0.9,
            'verb_compound': 0.85,
            'particle': 0.9,
            'special_pattern': 0.95,
            'character': 0.6
        }
        
        self.nnlds_weights = {
            'V1': 0.8,  # Foundation
            'V2': 0.7,  # Extension
            'V3': 0.9,  # Internal
            'V4': 0.85, # Closure
            'V5': 0.8,  # Direction
            'V6': 0.75  # Result
        }
        
        self.tone_weights = {
            'Neutral': 0.7,
            'á€·': 0.8,    # Short impulse
            'á€¹': 0.9,    # Checked stop
            'á€¸': 0.85    # Sustained
        }
    
    def get_attention_config(self, token_info: Dict) -> Dict:
        """Calculate attention configuration for token"""
        base_weight = self.base_weights.get(token_info.get('type', 'character'), 0.5)
        
        # Apply NNLDS-based adjustments
        nnlds_adjustment = self._calculate_nnlds_adjustment(token_info)
        
        # Apply contextual adjustments
        contextual_adjustment = self._calculate_contextual_adjustment(token_info)
        
        final_weight = base_weight * nnlds_adjustment * contextual_adjustment
        
        return {
            'base_weight': base_weight,
            'nnlds_adjustment': nnlds_adjustment,
            'contextual_adjustment': contextual_adjustment,
            'final_weight': min(final_weight, 1.0),  # Cap at 1.0
            'focus_type': self._determine_focus_type(token_info)
        }
    
    def _calculate_nnlds_adjustment(self, token_info: Dict) -> float:
        """Calculate adjustment based on NNLDS analysis"""
        adjustment = 1.0
        
        nnlds_data = token_info.get('nnlds_triplet', {})
        if not nnlds_data:
            return adjustment
            
        # Apply vowel species adjustment
        semantic_components = nnlds_data.get('semantic_components', {})
        m4_species = semantic_components.get('semantic_category')
        if m4_species:
            adjustment *= self.nnlds_weights.get(m4_species, 1.0)
            
        # Apply tone adjustment
        temporal_aspect = nnlds_data.get('temporal_aspect', {})
        m3_tone = temporal_aspect.get('aspect_type')
        if m3_tone:
            # Map aspect type to tone weights
            tone_mapping = {
                'immediate': 'á€·',
                'continuous': 'á€¸', 
                'completed': 'á€¹'
            }
            tone = tone_mapping.get(m3_tone, 'Neutral')
            adjustment *= self.tone_weights.get(tone, 1.0)
            
        return adjustment
    
    def _calculate_contextual_adjustment(self, token_info: Dict) -> float:
        """Calculate adjustment based on token context"""
        adjustment = 1.0
        
        token_type = token_info.get('type')
        
        # Higher weight for sentence-initial tokens
        if token_info.get('position', 0) == 0:
            adjustment *= 1.1
            
        # Higher weight for predicates
        if token_type in ['root_verb', 'verb_compound']:
            adjustment *= 1.15
            
        # Lower weight for common particles in middle position
        if token_type == 'particle' and token_info.get('position', 0) > 1:
            adjustment *= 0.9
            
        return adjustment
    
    def _determine_focus_type(self, token_info: Dict) -> str:
        """Determine the type of attention focus needed"""
        token_type = token_info.get('type', 'character')
        
        focus_types = {
            'root_noun': 'semantic_core',
            'compound_noun': 'semantic_unit',
            'root_verb': 'action_core', 
            'verb_compound': 'temporal_action',
            'particle': 'grammatical_relation',
            'special_pattern': 'fixed_expression',
            'character': 'basic_unit'
        }
        
        return focus_types.get(token_type, 'basic_unit')
    
    def generate_attention_map(self, tokens: List[Dict]) -> List[Dict]:
        """Generate attention map for sentence"""
        attention_map = []
        
        for i, token in enumerate(tokens):
            token['position'] = i  # Add position information
            attention_config = self.get_attention_config(token)
            
            attention_map.append({
                'token': token['token'],
                'type': token.get('type'),
                'group': token.get('group'),
                'attention_weight': attention_config['final_weight'],
                'focus_type': attention_config['focus_type'],
                'position': i
            })
            
        return attention_map
```

### 4. `src/utils.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Utility functions for Myanmar Language Processing
"""

import json
import re
from typing import List, Dict

def load_dictionary(file_path: str) -> Dict:
    """Load dictionary file"""
    with open(file_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def save_dictionary(data: Dict, file_path: str):
    """Save dictionary file"""
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def is_myanmar_text(text: str) -> bool:
    """Check if text contains Myanmar characters"""
    return bool(re.search(r'[\u1000-\u109F]', text))

def normalize_myanmar_text(text: str) -> str:
    """Normalize Myanmar text (basic normalization)"""
    # Add more normalization rules as needed
    replacements = {
        '\u1039\u1000': '\u1060',  # Kinzi normalization
        '\u1039\u1001': '\u1061',
        # Add more replacements
    }
    
    for old, new in replacements.items():
        text = text.replace(old, new)
        
    return text

def calculate_vocabulary_stats(tokens: List) -> Dict:
    """Calculate vocabulary statistics"""
    stats = {
        'total_tokens': len(tokens),
        'unique_tokens': len(set(tokens)),
        'token_frequency': {},
        'group_distribution': {}
    }
    
    for token in tokens:
        if isinstance(token, dict):
            token_text = token['token']
            group = token.get('group', 'unknown')
        else:
            token_text = token
            group = 'unknown'
            
        stats['token_frequency'][token_text] = stats['token_frequency'].get(token_text, 0) + 1
        stats['group_distribution'][group] = stats['group_distribution'].get(group, 0) + 1
        
    return stats
```

### 5. `data/sar_pone_gyi_dictionary.json`

```json
{
  "root_nouns": {
    "á€œá€°": {"group": "group_1", "semantic": "person"},
    "á€¡á€­á€™á€º": {"group": "group_1", "semantic": "house"},
    "á€…á€¬": {"group": "group_1", "semantic": "letter/book"},
    "á€›á€±": {"group": "group_1", "semantic": "water"},
    "á€™á€®á€¸": {"group": "group_1", "semantic": "fire"},
    "á€€á€»á€¬á€¸": {"group": "group_1", "semantic": "tiger/male"},
    "á€™": {"group": "group_1", "semantic": "female"},
    "á€€á€»á€±á€¬á€„á€ºá€¸": {"group": "group_1", "semantic": "school"},
    "á€™á€¼á€­á€¯á€·": {"group": "group_1", "semantic": "town"}
  },
  "compound_nouns": {
    "á€…á€€á€ºá€˜á€®á€¸": {"group": "group_2", "semantic": "bicycle"},
    "á€›á€”á€ºá€€á€¯á€”á€º": {"group": "group_2", "semantic": "Yangon"},
    "á€™á€­á€¯á€¸á€œá€±á€á€": {"group": "group_2", "semantic": "meteorology"},
    "á€™á€®á€¸á€›á€‘á€¬á€¸": {"group": "group_2", "semantic": "train"},
    "á€œá€±á€šá€¬á€‰á€º": {"group": "group_2", "semantic": "airplane"},
    "á€€á€»á€±á€¬á€„á€ºá€¸á€á€¬á€¸": {"group": "group_2", "semantic": "student"},
    "á€†á€±á€¸á€›á€¯á€¶": {"group": "group_2", "semantic": "hospital"},
    "á€˜á€á€º": {"group": "group_2", "semantic": "bank"}
  },
  "root_verbs": {
    "á€…á€¬á€¸": {"group": "group_3", "semantic": "eat"},
    "á€á€½á€¬á€¸": {"group": "group_3", "semantic": "go"},
    "á€œá€¯á€•á€º": {"group": "group_3", "semantic": "do"},
    "á€–á€á€º": {"group": "group_3", "semantic": "read"},
    "á€›á€±á€¸": {"group": "group_3", "semantic": "write"},
    "á€€á€…á€¬á€¸": {"group": "group_3", "semantic": "play"},
    "á€€á€¼á€¬á€¸": {"group": "group_3", "semantic": "hear"},
    "á€•á€¼á€±á€¬": {"group": "group_3", "semantic": "speak"}
  },
  "verb_compounds": {
    "á€á€½á€¬á€¸á€™á€šá€º": {"group": "group_4", "semantic": "will go", "tense": "future"},
    "á€œá€¯á€•á€ºá€á€šá€º": {"group": "group_4", "semantic": "do (present)", "tense": "present"},
    "á€…á€¬á€¸á€•á€¼á€®á€¸": {"group": "group_4", "semantic": "after eating", "aspect": "perfective"},
    "á€€á€¼á€Šá€·á€ºá€›á€á€¬": {"group": "group_4", "semantic": "looking at", "aspect": "continuous"},
    "á€•á€¼á€±á€¬á€á€²á€·": {"group": "group_4", "semantic": "spoke", "tense": "past"},
    "á€œá€¬á€•á€«": {"group": "group_4", "semantic": "please come", "modality": "polite"}
  },
  "particles": {
    "á€€": {"group": "group_5", "role": "subject_marker"},
    "á€™á€¾á€¬": {"group": "group_5", "role": "location_marker"},
    "á": {"group": "group_5", "role": "possessive_marker"},
    "á€á€Šá€º": {"group": "group_5", "role": "topic_marker"},
    "á€”á€²á€·": {"group": "group_5", "role": "with_marker"},
    "á€€á€­á€¯": {"group": "group_5", "role": "object_marker"},
    "á€¡á€¬á€¸": {"group": "group_5", "role": "object_marker_formal"},
    "á€™á€¾": {"group": "group_5", "role": "from_marker"},
    "á€›á€²á€·": {"group": "group_5", "role": "possessive_marker_informal"}
  },
  "special_patterns": {
    "á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€á€šá€º": {"group": "group_6", "semantic": "can be", "modality": "possibility"},
    "á€™á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€˜á€°á€¸": {"group": "group_6", "semantic": "cannot be", "modality": "impossibility"},
    "á€á€½á€¬á€¸á€›á€™á€šá€º": {"group": "group_6", "semantic": "must go", "modality": "obligation"},
    "á€œá€¯á€•á€ºá€á€»á€„á€ºá€á€šá€º": {"group": "group_6", "semantic": "want to do", "modality": "desire"},
    "á€”á€»á€°á€€á€œá€®á€¸á€šá€¬á€¸": {"group": "group_6", "semantic": "nuclear", "domain": "technical"}
  }
}
```

### 6. `examples/basic_usage.py`

```python
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Basic usage examples for Myanmar Tokenizer
"""

import os
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from src.tokenizer import MyanmarTokenizer

def main():
    # Initialize tokenizer
    tokenizer = MyanmarTokenizer(
        nnlds_csv_path='../data/nnlds_codebook.csv',
        dictionary_path='../data/sar_pone_gyi_dictionary.json'
    )
    
    # Example sentences
    sentences = [
        "á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€…á€€á€ºá€˜á€®á€¸á€”á€²á€·á€á€½á€¬á€¸á€™á€šá€º",
        "á€™á€”á€±á€·á€€á€…á€¬á€¡á€¯á€•á€ºá€–á€á€ºá€á€²á€·á€á€šá€º",
        "á€á€°á€€á€€á€»á€±á€¬á€„á€ºá€¸á€á€¬á€¸á€–á€¼á€…á€ºá€”á€­á€¯á€„á€ºá€á€šá€º"
    ]
    
    print("Myanmar Language Tokenizer Demo")
    print("=" * 50)
    
    for i, sentence in enumerate(sentences, 1):
        print(f"\n{i}. Sentence: {sentence}")
        print("-" * 30)
        
        # Basic tokenization
        tokens = tokenizer.tokenize(sentence)
        print(f"Tokens: {tokens}")
        
        # Detailed analysis
        analysis = tokenizer.analyze_sentence(sentence)
        
        print(f"Linguistic Analysis:")
        print(f"  Subject: {analysis['linguistic_analysis']['subject']}")
        print(f"  Verb: {analysis['linguistic_analysis']['verb']}")
        print(f"  Object: {analysis['linguistic_analysis']['object']}")
        print(f"  Particles: {len(analysis['linguistic_analysis']['particles'])}")
        
        print(f"Attention Map:")
        for attn in analysis['attention_map']:
            print(f"  {attn['token']}: weight={attn['attention_weight']:.2f}, type={attn['focus_type']}")

if __name__ == "__main__":
    main()
```

### 7. `requirements.txt`

```
pandas>=1.3.0
numpy>=1.21.0
regex>=2021.0.0
```

### 8. `README.md`

```markdown
# Myanmar Language Tokenization System

A comprehensive tokenization system for the Myanmar language combining traditional Sar Pone Gyi linguistic methods with modern computational approaches using the NNLDS code system.

## Features

- **Linguistically-Aware Tokenization**: Based on Sar Pone Gyi method with 6 token groups
- **Phonetic-Semantic Analysis**: Integrated NNLDS code system for detailed linguistic analysis
- **Attention Mechanism**: Custom attention weights based on linguistic importance
- **Modular Architecture**: Easy to extend and customize

## Installation

```bash
git clone https://github.com/your-username/myanmar-tokenizer.git
cd myanmar-tokenizer
pip install -r requirements.txt
```

## Quick Start

```python
from src.tokenizer import MyanmarTokenizer

# Initialize tokenizer
tokenizer = MyanmarTokenizer(
    nnlds_csv_path='data/nnlds_codebook.csv',
    dictionary_path='data/sar_pone_gyi_dictionary.json'
)

# Tokenize sentence
sentence = "á€€á€»á€½á€”á€ºá€á€±á€¬á€ºá€…á€¬á€¡á€¯á€•á€ºá€–á€á€ºá€á€šá€º"
tokens = tokenizer.tokenize(sentence)
print(tokens)

# Detailed analysis
analysis = tokenizer.analyze_sentence(sentence)
print(analysis)
```

## Token Groups

1. **Root Nouns** (á€•á€„á€ºá€€á€­á€¯á€šá€ºá€”á€¬á€™á€ºá€™á€»á€¬á€¸)
2. **Compound Nouns** (á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€”á€¬á€™á€ºá€™á€»á€¬á€¸) 
3. **Root Verbs** (á€€á€¼á€­á€šá€¬á€¡á€™á€¼á€…á€ºá€™á€»á€¬á€¸)
4. **Verb Compounds** (á€€á€¼á€­á€šá€¬á€•á€±á€«á€„á€ºá€¸á€…á€•á€ºá€™á€»á€¬á€¸)
5. **Particles** (á€•á€…á€¹á€…á€Šá€ºá€¸á€†á€€á€ºá€™á€»á€¬á€¸)
6. **Special Patterns** (á€¡á€‘á€°á€¸á€•á€¯á€¶á€…á€¶á€™á€»á€¬á€¸)

## NNLDS Integration

The system uses the NNLDS (Nissaya Numerical Linguistic Data System) code system for:

- Phonetic analysis and breakdown
- Semantic role assignment  
- Temporal and aspectual information
- Attention weight calculation

## License

MIT License

## Contributing

Contributions welcome! Please see CONTRIBUTING.md for details.
```

## Complete GitHub Setup

á€’á€® codebase á€€á€­á€¯ GitHub á€™á€¾á€¬ á€á€„á€ºá€–á€­á€¯á€·á€¡á€á€½á€€á€º á€”á€±á€¬á€€á€ºá€‘á€•á€ºá€œá€­á€¯á€¡á€•á€ºá€á€²á€· file á€á€½á€±á€€á€­á€¯ á€‘á€Šá€·á€ºá€•á€±á€¸á€‘á€¬á€¸á€•á€«á€á€šá€ºá‹ á€€á€»á€±á€¸á€‡á€°á€¸á€•á€¼á€¯á€•á€¼á€®á€¸ á€’á€®á€œá€­á€¯á€á€±á€¬á€„á€ºá€¸á€†á€­á€¯á€™á€¾á€¯á€€á€­á€¯ á€œá€­á€¯á€€á€ºá€œá€¯á€•á€ºá€•á€±á€¸á€á€²á€·á€¡á€á€½á€€á€º á€¡á€‘á€°á€¸á€€á€»á€±á€¸á€‡á€°á€¸á€á€„á€ºá€•á€«á€á€šá€º! ğŸŒŸ

á€™á€„á€ºá€¸á€›á€²á€· á€€á€»á€±á€¸á€‡á€°á€¸á€€á€¼á€±á€¬á€„á€·á€º á€™á€¼á€”á€ºá€™á€¬á€…á€¬ NLP community á€¡á€á€½á€€á€º á€€á€±á€¬á€„á€ºá€¸á€™á€½á€”á€ºá€á€²á€· tool á€á€…á€ºá€á€¯á€€á€­á€¯ á€™á€»á€¾á€á€±á€”á€­á€¯á€„á€ºá€á€±á€¬á€·á€™á€¾á€¬á€•á€«á‹
